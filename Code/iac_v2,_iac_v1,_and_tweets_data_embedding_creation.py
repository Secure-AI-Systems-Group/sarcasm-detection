# -*- coding: utf-8 -*-
"""IAC-V2, IAC-V1, and Tweets Data/Embedding Creation

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1j4FWLjCg2M9JbPnR3fqzFVxbopX2ojTY

## Imports:
"""

from google.colab import drive
drive.mount('/content/drive')

!pip install transformers
!pip install torch
!pip install pytorch_metric_learning

import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.utils.data import DataLoader, TensorDataset
import pandas as pd

"""## IAC-V2:

### Dataframe Formatting:
"""

train_v2 = pd.read_csv('/content/drive/MyDrive/Contrastive NLP Stuff/Data/SOTA Data/CSV Data/IAC-V2/train.csv')
test_v2 = pd.read_csv('/content/drive/MyDrive/Contrastive NLP Stuff/Data/SOTA Data/CSV Data/IAC-V2/test.csv')
val_v2 = pd.read_csv('/content/drive/MyDrive/Contrastive NLP Stuff/Data/SOTA Data/CSV Data/IAC-V2/valid.csv')

# Format the dataframe to match the earlier SoTA specifications
train_v2.rename(columns={'text': 'tweet', 'label': 'sarcastic'}, inplace=True)
test_v2.rename(columns={'text': 'tweet', 'label': 'sarcastic'}, inplace=True)
val_v2.rename(columns={'text': 'tweet', 'label': 'sarcastic'}, inplace=True)

train_v2['sarcastic'] = train_v2['sarcastic'].map({'notsarc': 0, 'sarc': 1})
test_v2['sarcastic'] = test_v2['sarcastic'].map({'notsarc': 0, 'sarc': 1})
val_v2['sarcastic'] = val_v2['sarcastic'].map({'notsarc': 0, 'sarc': 1})

train_v2.to_csv('train.csv')
test_v2.to_csv('test.csv')
val_v2.to_csv('val.csv')

import shutil
shutil.copy("/content/train.csv","/content/drive/MyDrive/Contrastive NLP Stuff/Data/SOTA Data/CSV Data/Formatted IAC-V2")
shutil.copy("/content/test.csv","/content/drive/MyDrive/Contrastive NLP Stuff/Data/SOTA Data/CSV Data/Formatted IAC-V2")
shutil.copy("/content/val.csv","/content/drive/MyDrive/Contrastive NLP Stuff/Data/SOTA Data/CSV Data/Formatted IAC-V2")

"""### BERTweet Sentence Embeddings:"""

!pip install emoji

from transformers import AutoModel, AutoTokenizer

# Load pre-trained model and tokenizer
MODEL = "vinai/bertweet-base"
tokenizer = AutoTokenizer.from_pretrained(MODEL)
model = AutoModel.from_pretrained(MODEL)

plain_text_list = list(train_v2['tweet'])
train_embeddings = []

# Generate embeddings for each text in the list
for text in plain_text_list:
  if not isinstance(text, str):
    text = str(text)
  text_input = tokenizer(text, return_tensors="pt", padding='longest', truncation=True)
  with torch.no_grad():
    outputs = model(**text_input)
  last_hidden_states = outputs[0]
  sentence_embedding = torch.mean(last_hidden_states, dim=1)
  train_embeddings.append(sentence_embedding[0].detach().cpu())

from transformers import AutoModel, AutoTokenizer

# Load pre-trained model and tokenizer
MODEL = "vinai/bertweet-base"
tokenizer = AutoTokenizer.from_pretrained(MODEL)
model = AutoModel.from_pretrained(MODEL)

plain_text_list = list(test_v2['tweet'])
test_embeddings = []

# Generate embeddings for each text in the list
for text in plain_text_list:
  if not isinstance(text, str):
    text = str(text)
  text_input = tokenizer(text, return_tensors="pt", padding='longest', truncation=True)
  with torch.no_grad():
    outputs = model(**text_input)
  last_hidden_states = outputs[0]
  sentence_embedding = torch.mean(last_hidden_states, dim=1)
  test_embeddings.append(sentence_embedding[0].detach().cpu())

from transformers import AutoModel, AutoTokenizer

# Load pre-trained model and tokenizer
MODEL = "vinai/bertweet-base"
tokenizer = AutoTokenizer.from_pretrained(MODEL)
model = AutoModel.from_pretrained(MODEL)

plain_text_list = list(val_v2['tweet'])
val_embeddings = []

# Generate embeddings for each text in the list
for text in plain_text_list:
  if not isinstance(text, str):
    text = str(text)
  text_input = tokenizer(text, return_tensors="pt", padding='longest', truncation=True)
  with torch.no_grad():
    outputs = model(**text_input)
  last_hidden_states = outputs[0]
  sentence_embedding = torch.mean(last_hidden_states, dim=1)
  val_embeddings.append(sentence_embedding[0].detach().cpu())

"""#### Upload to Drive:"""

import pickle
with open("/content/drive/MyDrive/Contrastive NLP Stuff/Data/SOTA Data/Embeddings/IAC-V2/BERTweet_train", "wb") as fp:
  pickle.dump(train_embeddings, fp)

import pickle
with open("/content/drive/MyDrive/Contrastive NLP Stuff/Data/SOTA Data/Embeddings/IAC-V2/BERTweet_test", "wb") as fp:
  pickle.dump(test_embeddings, fp)

import pickle
with open("/content/drive/MyDrive/Contrastive NLP Stuff/Data/SOTA Data/Embeddings/IAC-V2/BERTweet_val", "wb") as fp:
  pickle.dump(val_embeddings, fp)

"""### BERTweet SimCLR Sentence-Level Embeddings:

#### BERTweet Model (Trained via SimCLR Framework):
"""

class config:
  batch_size = 50
  num_workers = 4
  text_encoder_lr = 1e-5
  weight_decay = 1e-3
  epochs = 10
  device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

  model_name = 'vinai/bertweet-base'
  text_embedding = 768
  max_length = 510
  embedding_size = 768
  hidden_size = 256

  pretrained = True
  trainable_language = True
  trainable_logic = True
  temperature = .7

  num_projection_layers = 2
  projection_dim = 256
  dropout = 0.01

from transformers import RobertaModel, RobertaTokenizer
# Used to encode the inputs using RoBERTa

class Encoder(torch.nn.Module):
  def __init__(self, model_name, trainable):
    super().__init__()
    self.model = RobertaModel.from_pretrained(model_name)
    for param in self.model.parameters():
      param.requires_grad = trainable

  def forward(self, input_ids, attention_mask):
    outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)
    last_hidden_state = outputs[0]
    pooled_output = last_hidden_state.mean(dim=1)
    return pooled_output

# Used to reduce the dimensionality of RoBERTa outputs and get it ready for classification task

class ProjectionHead(torch.nn.Module):
  def __init__(self, input_dim, output_dim, dropout_rate):
    super().__init__()

    self.projection = torch.nn.Sequential(
      torch.nn.Linear(input_dim, output_dim),
      torch.nn.ReLU(),
      torch.nn.Dropout(dropout_rate),
      torch.nn.Linear(output_dim, output_dim)
    )

  def forward(self, x):
    return self.projection(x)

from pytorch_metric_learning.losses import NTXentLoss
# Applies the SimCLR framework and creates a SimCLR model on the data

class SimCLR(nn.Module):
  def __init__(self, encoder, projection_head, temperature):
    super(SimCLR, self).__init__()
    self.encoder = encoder
    self.projection_head = projection_head
    self.temperature = temperature
    self.loss_fn = NTXentLoss(temperature)

  def forward(self, anchor_input, positive_input, negative_input):
    z_i = self.projection_head(self.encoder(**anchor_input))
    z_j = self.projection_head(self.encoder(**positive_input))
    z_k = self.projection_head(self.encoder(**negative_input))
    out = torch.cat([z_i, z_j], dim=0)
    labels = torch.cat([torch.arange(z_i.size(0)), torch.arange(z_i.size(0))], dim=0).to(z_i.device)

    return out, labels

  def get_loss(self, anchor_input, positive_input, negative_input):
    out, labels = self.forward(anchor_input, positive_input, negative_input)
    loss = self.loss_fn(out, labels)
    return loss

!gdown https://drive.google.com/uc?id=1NmF-x91w1uimC_cTOtykGJuw1SGZwo20
# Instantiate the SimCLR model
encoder = Encoder(config.model_name, config.trainable_language)
projection_head = ProjectionHead(config.text_embedding, config.projection_dim, config.dropout)
simclr_model = SimCLR(encoder, projection_head, config.temperature)
state_dict = torch.load("BERTweet_simclr_model_epoch_10.pth")
simclr_model.load_state_dict(state_dict)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

"""#### Generation:"""

!pip install emoji

simclr_model = simclr_model.to(config.device)
from transformers import AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained(config.model_name)

plain_text_list = list(train_v2['tweet'])
train_embeddings = []

# Generate embeddings for each text in the list
for text in plain_text_list:
  if not isinstance(text, str):
    text = str(text)
  text_input = {key: val.to(config.device) for key, val in tokenizer(text, return_tensors="pt", padding='longest', truncation=True).items() if key != 'token_type_ids'}
  text_embeddings = simclr_model.encoder(**text_input)
  train_embeddings.append(text_embeddings[0].detach().cpu())

simclr_model = simclr_model.to(config.device)
from transformers import AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained(config.model_name)

plain_text_list = list(test_v2['tweet'])
test_embeddings = []

# Generate embeddings for each text in the list
for text in plain_text_list:
  if not isinstance(text, str):
    text = str(text)
  text_input = {key: val.to(config.device) for key, val in tokenizer(text, return_tensors="pt", padding='longest', truncation=True).items() if key != 'token_type_ids'}
  text_embeddings = simclr_model.encoder(**text_input)
  test_embeddings.append(text_embeddings[0].detach().cpu())

simclr_model = simclr_model.to(config.device)
from transformers import AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained(config.model_name)

plain_text_list = list(val_v2['tweet'])
val_embeddings = []

# Generate embeddings for each text in the list
for text in plain_text_list:
  if not isinstance(text, str):
    text = str(text)
  text_input = {key: val.to(config.device) for key, val in tokenizer(text, return_tensors="pt", padding='longest', truncation=True).items() if key != 'token_type_ids'}
  text_embeddings = simclr_model.encoder(**text_input)
  val_embeddings.append(text_embeddings[0].detach().cpu())

"""#### Upload to Drive:"""

import pickle
with open("/content/drive/MyDrive/Contrastive NLP Stuff/Data/SOTA Data/Embeddings/IAC-V2/train_BERTweet_sentence", "wb") as fp:
  pickle.dump(train_embeddings, fp)

import pickle
with open("/content/drive/MyDrive/Contrastive NLP Stuff/Data/SOTA Data/Embeddings/IAC-V2/test_BERTweet_sentence", "wb") as fp:
  pickle.dump(test_embeddings, fp)

import pickle
with open("/content/drive/MyDrive/Contrastive NLP Stuff/Data/SOTA Data/Embeddings/IAC-V2/val_BERTweet_sentence", "wb") as fp:
  pickle.dump(val_embeddings, fp)

"""### Vanilla Embeddings:"""

from transformers import RobertaModel, RobertaTokenizer

# Load pre-trained model and tokenizer
tokenizer = RobertaTokenizer.from_pretrained('roberta-base')
model = RobertaModel.from_pretrained('roberta-base')

plain_text_list = list(train_v2['tweet'])
vanilla_train_embeddings = []

# Generate embeddings for each text in the list
for text in plain_text_list:
  if not isinstance(text, str):
    text = str(text)
  text_input = tokenizer(text, return_tensors="pt", padding='longest', truncation=True)
  with torch.no_grad():
    outputs = model(**text_input)
  last_hidden_states = outputs[0]
  sentence_embedding = torch.mean(last_hidden_states, dim=1)
  vanilla_train_embeddings.append(sentence_embedding[0].detach().cpu())

from transformers import RobertaModel, RobertaTokenizer

# Load pre-trained model and tokenizer
tokenizer = RobertaTokenizer.from_pretrained('roberta-base')
model = RobertaModel.from_pretrained('roberta-base')

plain_text_list = list(test_v2['tweet'])
vanilla_test_embeddings = []

# Generate embeddings for each text in the list
for text in plain_text_list:
  if not isinstance(text, str):
    text = str(text)
  text_input = tokenizer(text, return_tensors="pt", padding='longest', truncation=True)
  with torch.no_grad():
    outputs = model(**text_input)
  last_hidden_states = outputs[0]
  sentence_embedding = torch.mean(last_hidden_states, dim=1)
  vanilla_test_embeddings.append(sentence_embedding[0].detach().cpu())

from transformers import RobertaModel, RobertaTokenizer

# Load pre-trained model and tokenizer
tokenizer = RobertaTokenizer.from_pretrained('roberta-base')
model = RobertaModel.from_pretrained('roberta-base')

plain_text_list = list(val_v2['tweet'])
vanilla_val_embeddings = []

# Generate embeddings for each text in the list
for text in plain_text_list:
  if not isinstance(text, str):
    text = str(text)
  text_input = tokenizer(text, return_tensors="pt", padding='longest', truncation=True)
  with torch.no_grad():
    outputs = model(**text_input)
  last_hidden_states = outputs[0]
  sentence_embedding = torch.mean(last_hidden_states, dim=1)
  vanilla_val_embeddings.append(sentence_embedding[0].detach().cpu())

"""#### Upload to Drive:"""

import pickle
with open("/content/drive/MyDrive/Contrastive NLP Stuff/Data/SOTA Data/Embeddings/IAC-V2/train_vanilla", "wb") as fp:
  pickle.dump(vanilla_train_embeddings, fp)

import pickle
with open("/content/drive/MyDrive/Contrastive NLP Stuff/Data/SOTA Data/Embeddings/IAC-V2/test_vanilla", "wb") as fp:
  pickle.dump(vanilla_test_embeddings, fp)

import pickle
with open("/content/drive/MyDrive/Contrastive NLP Stuff/Data/SOTA Data/Embeddings/IAC-V2/val_vanilla", "wb") as fp:
  pickle.dump(vanilla_val_embeddings, fp)

"""### Word Embeddings:"""

import nltk
nltk.download('stopwords')
nltk.download('punkt')

import re
import numpy as np
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from gensim.models import Word2Vec

stop_words = set(stopwords.words('english'))

def preprocess_text(text):
  text = text.lower()
  text = re.sub(r'[^\w\s]', '', text)
  tokens = word_tokenize(text)
  tokens = [token for token in tokens if token not in stop_words]
  return tokens

train_texts = [preprocess_text(text) for text in train_v2['tweet']]
test_texts = [preprocess_text(text) for text in test_v2['tweet']]
val_texts = [preprocess_text(text) for text in val_v2['tweet']]

model = Word2Vec(sentences=train_texts, vector_size=768, window=5, min_count=1, workers=4)

model.save("word2vec.model")

def text_to_embedding(text, model, max_len):
  embeddings = [model.wv[word] for word in text if word in model.wv]
  if len(embeddings) < max_len:
    embeddings = np.pad(embeddings, ((0, max_len - len(embeddings)), (0, 0)), mode='constant')
  else:
    embeddings = embeddings[:max_len]
  return embeddings

max_len = 50
train_embeddings = [text_to_embedding(text, model, max_len) for text in train_texts]
test_embeddings = [text_to_embedding(text, model, max_len) for text in test_texts]
val_embeddings = [text_to_embedding(text, model, max_len) for text in val_texts]

"""#### Upload to Drive:"""

import pickle
with open("/content/drive/MyDrive/Contrastive NLP Stuff/Data/SOTA Data/Embeddings/IAC-V2/train_word", "wb") as fp:
  pickle.dump(train_embeddings, fp)

import pickle
with open("/content/drive/MyDrive/Contrastive NLP Stuff/Data/SOTA Data/Embeddings/IAC-V2/test_word", "wb") as fp:
  pickle.dump(test_embeddings, fp)

import pickle
with open("/content/drive/MyDrive/Contrastive NLP Stuff/Data/SOTA Data/Embeddings/IAC-V2/val_word", "wb") as fp:
  pickle.dump(val_embeddings, fp)

"""## IAC-V1:

### Dataframe Formatting:
"""

train_v1 = pd.read_csv('/content/drive/MyDrive/Contrastive NLP Stuff/Data/SOTA Data/CSV Data/IAC-V1/train.csv')
test_v1 = pd.read_csv('/content/drive/MyDrive/Contrastive NLP Stuff/Data/SOTA Data/CSV Data/IAC-V1/test.csv')
val_v1 = pd.read_csv('/content/drive/MyDrive/Contrastive NLP Stuff/Data/SOTA Data/CSV Data/IAC-V1/valid.csv')

# Format the dataframe to match the earlier SoTA specifications
train_v1.rename(columns={'text': 'tweet', 'label': 'sarcastic'}, inplace=True)
test_v1.rename(columns={'text': 'tweet', 'label': 'sarcastic'}, inplace=True)
val_v1.rename(columns={'text': 'tweet', 'label': 'sarcastic'}, inplace=True)

train_v1['sarcastic'] = train_v1['sarcastic'].map({'notsarc': 0, 'sarc': 1})
test_v1['sarcastic'] = test_v1['sarcastic'].map({'notsarc': 0, 'sarc': 1})
val_v1['sarcastic'] = val_v1['sarcastic'].map({'notsarc': 0, 'sarc': 1})

train_v1.to_csv('train.csv')
test_v1.to_csv('test.csv')
val_v1.to_csv('val.csv')

import shutil
shutil.copy("/content/train.csv","/content/drive/MyDrive/Contrastive NLP Stuff/Data/SOTA Data/CSV Data/Formatted IAC-V1")
shutil.copy("/content/test.csv","/content/drive/MyDrive/Contrastive NLP Stuff/Data/SOTA Data/CSV Data/Formatted IAC-V1")
shutil.copy("/content/val.csv","/content/drive/MyDrive/Contrastive NLP Stuff/Data/SOTA Data/CSV Data/Formatted IAC-V1")

"""### BERTweet Sentence Embeddings:"""

!pip install emoji

from transformers import AutoModel, AutoTokenizer

# Load pre-trained model and tokenizer
MODEL = "vinai/bertweet-base"
tokenizer = AutoTokenizer.from_pretrained(MODEL)
model = AutoModel.from_pretrained(MODEL)

plain_text_list = list(train_v1['tweet'])
train_embeddings = []

# Generate embeddings for each text in the list
for text in plain_text_list:
  if not isinstance(text, str):
    text = str(text)
  text_input = tokenizer(text, return_tensors="pt", padding='longest', truncation=True)
  with torch.no_grad():
    outputs = model(**text_input)
  last_hidden_states = outputs[0]
  sentence_embedding = torch.mean(last_hidden_states, dim=1)
  train_embeddings.append(sentence_embedding[0].detach().cpu())

from transformers import AutoModel, AutoTokenizer

# Load pre-trained model and tokenizer
MODEL = "vinai/bertweet-base"
tokenizer = AutoTokenizer.from_pretrained(MODEL)
model = AutoModel.from_pretrained(MODEL)

plain_text_list = list(test_v1['tweet'])
test_embeddings = []

# Generate embeddings for each text in the list
for text in plain_text_list:
  if not isinstance(text, str):
    text = str(text)
  text_input = tokenizer(text, return_tensors="pt", padding='longest', truncation=True)
  with torch.no_grad():
    outputs = model(**text_input)
  last_hidden_states = outputs[0]
  sentence_embedding = torch.mean(last_hidden_states, dim=1)
  test_embeddings.append(sentence_embedding[0].detach().cpu())

from transformers import AutoModel, AutoTokenizer

# Load pre-trained model and tokenizer
MODEL = "vinai/bertweet-base"
tokenizer = AutoTokenizer.from_pretrained(MODEL)
model = AutoModel.from_pretrained(MODEL)

plain_text_list = list(val_v1['tweet'])
val_embeddings = []

# Generate embeddings for each text in the list
for text in plain_text_list:
  if not isinstance(text, str):
    text = str(text)
  text_input = tokenizer(text, return_tensors="pt", padding='longest', truncation=True)
  with torch.no_grad():
    outputs = model(**text_input)
  last_hidden_states = outputs[0]
  sentence_embedding = torch.mean(last_hidden_states, dim=1)
  val_embeddings.append(sentence_embedding[0].detach().cpu())

"""#### Upload to Drive:"""

import pickle
with open("/content/drive/MyDrive/Contrastive NLP Stuff/Data/SOTA Data/Embeddings/IAC-V1/BERTweet_train", "wb") as fp:
  pickle.dump(train_embeddings, fp)

import pickle
with open("/content/drive/MyDrive/Contrastive NLP Stuff/Data/SOTA Data/Embeddings/IAC-V1/BERTweet_test", "wb") as fp:
  pickle.dump(test_embeddings, fp)

import pickle
with open("/content/drive/MyDrive/Contrastive NLP Stuff/Data/SOTA Data/Embeddings/IAC-V1/BERTweet_val", "wb") as fp:
  pickle.dump(val_embeddings, fp)

"""### BERTweet SimCLR Sentence-Level Embeddings:

#### BERTweet Model (Trained via SimCLR Framework):
"""

class config:
  batch_size = 50
  num_workers = 4
  text_encoder_lr = 1e-5
  weight_decay = 1e-3
  epochs = 10
  device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

  model_name = 'vinai/bertweet-base'
  text_embedding = 768
  max_length = 510
  embedding_size = 768
  hidden_size = 256

  pretrained = True
  trainable_language = True
  trainable_logic = True
  temperature = .7

  num_projection_layers = 2
  projection_dim = 256
  dropout = 0.01

from transformers import RobertaModel, RobertaTokenizer
# Used to encode the inputs using RoBERTa

class Encoder(torch.nn.Module):
  def __init__(self, model_name, trainable):
    super().__init__()
    self.model = RobertaModel.from_pretrained(model_name)
    for param in self.model.parameters():
      param.requires_grad = trainable

  def forward(self, input_ids, attention_mask):
    outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)
    last_hidden_state = outputs[0]
    pooled_output = last_hidden_state.mean(dim=1)
    return pooled_output

# Used to reduce the dimensionality of RoBERTa outputs and get it ready for classification task

class ProjectionHead(torch.nn.Module):
  def __init__(self, input_dim, output_dim, dropout_rate):
    super().__init__()

    self.projection = torch.nn.Sequential(
      torch.nn.Linear(input_dim, output_dim),
      torch.nn.ReLU(),
      torch.nn.Dropout(dropout_rate),
      torch.nn.Linear(output_dim, output_dim)
    )

  def forward(self, x):
    return self.projection(x)

from pytorch_metric_learning.losses import NTXentLoss
# Applies the SimCLR framework and creates a SimCLR model on the data

class SimCLR(nn.Module):
  def __init__(self, encoder, projection_head, temperature):
    super(SimCLR, self).__init__()
    self.encoder = encoder
    self.projection_head = projection_head
    self.temperature = temperature
    self.loss_fn = NTXentLoss(temperature)

  def forward(self, anchor_input, positive_input, negative_input):
    z_i = self.projection_head(self.encoder(**anchor_input))
    z_j = self.projection_head(self.encoder(**positive_input))
    z_k = self.projection_head(self.encoder(**negative_input))
    out = torch.cat([z_i, z_j], dim=0)
    labels = torch.cat([torch.arange(z_i.size(0)), torch.arange(z_i.size(0))], dim=0).to(z_i.device)

    return out, labels

  def get_loss(self, anchor_input, positive_input, negative_input):
    out, labels = self.forward(anchor_input, positive_input, negative_input)
    loss = self.loss_fn(out, labels)
    return loss

!gdown https://drive.google.com/uc?id=1NmF-x91w1uimC_cTOtykGJuw1SGZwo20
# Instantiate the SimCLR model
encoder = Encoder(config.model_name, config.trainable_language)
projection_head = ProjectionHead(config.text_embedding, config.projection_dim, config.dropout)
simclr_model = SimCLR(encoder, projection_head, config.temperature)
state_dict = torch.load("BERTweet_simclr_model_epoch_10.pth")
simclr_model.load_state_dict(state_dict)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

"""#### Generation:"""

!pip install emoji

simclr_model = simclr_model.to(config.device)
from transformers import AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained(config.model_name)

plain_text_list = list(train_v1['tweet'])
train_embeddings = []

# Generate embeddings for each text in the list
for text in plain_text_list:
  if not isinstance(text, str):
    text = str(text)
  text_input = {key: val.to(config.device) for key, val in tokenizer(text, return_tensors="pt", padding='longest', truncation=True).items() if key != 'token_type_ids'}
  text_embeddings = simclr_model.encoder(**text_input)
  train_embeddings.append(text_embeddings[0].detach().cpu())

simclr_model = simclr_model.to(config.device)
from transformers import AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained(config.model_name)

plain_text_list = list(test_v1['tweet'])
test_embeddings = []

# Generate embeddings for each text in the list
for text in plain_text_list:
  if not isinstance(text, str):
    text = str(text)
  text_input = {key: val.to(config.device) for key, val in tokenizer(text, return_tensors="pt", padding='longest', truncation=True).items() if key != 'token_type_ids'}
  text_embeddings = simclr_model.encoder(**text_input)
  test_embeddings.append(text_embeddings[0].detach().cpu())

simclr_model = simclr_model.to(config.device)
from transformers import AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained(config.model_name)

plain_text_list = list(val_v1['tweet'])
val_embeddings = []

# Generate embeddings for each text in the list
for text in plain_text_list:
  if not isinstance(text, str):
    text = str(text)
  text_input = {key: val.to(config.device) for key, val in tokenizer(text, return_tensors="pt", padding='longest', truncation=True).items() if key != 'token_type_ids'}
  text_embeddings = simclr_model.encoder(**text_input)
  val_embeddings.append(text_embeddings[0].detach().cpu())

"""#### Upload to Drive:"""

import pickle
with open("/content/drive/MyDrive/Contrastive NLP Stuff/Data/SOTA Data/Embeddings/IAC-V1/train_BERTweet_sentence", "wb") as fp:
  pickle.dump(train_embeddings, fp)

import pickle
with open("/content/drive/MyDrive/Contrastive NLP Stuff/Data/SOTA Data/Embeddings/IAC-V1/test_BERTweet_sentence", "wb") as fp:
  pickle.dump(test_embeddings, fp)

import pickle
with open("/content/drive/MyDrive/Contrastive NLP Stuff/Data/SOTA Data/Embeddings/IAC-V1/val_BERTweet_sentence", "wb") as fp:
  pickle.dump(val_embeddings, fp)

"""### Vanilla Embeddings:"""

from transformers import RobertaModel, RobertaTokenizer

# Load pre-trained model and tokenizer
tokenizer = RobertaTokenizer.from_pretrained('roberta-base')
model = RobertaModel.from_pretrained('roberta-base')

plain_text_list = list(train_v1['tweet'])
vanilla_train_embeddings = []

# Generate embeddings for each text in the list
for text in plain_text_list:
  if not isinstance(text, str):
    text = str(text)
  text_input = tokenizer(text, return_tensors="pt", padding='longest', truncation=True)
  with torch.no_grad():
    outputs = model(**text_input)
  last_hidden_states = outputs[0]
  sentence_embedding = torch.mean(last_hidden_states, dim=1)
  vanilla_train_embeddings.append(sentence_embedding[0].detach().cpu())

from transformers import RobertaModel, RobertaTokenizer

# Load pre-trained model and tokenizer
tokenizer = RobertaTokenizer.from_pretrained('roberta-base')
model = RobertaModel.from_pretrained('roberta-base')

plain_text_list = list(test_v1['tweet'])
vanilla_test_embeddings = []

# Generate embeddings for each text in the list
for text in plain_text_list:
  if not isinstance(text, str):
    text = str(text)
  text_input = tokenizer(text, return_tensors="pt", padding='longest', truncation=True)
  with torch.no_grad():
    outputs = model(**text_input)
  last_hidden_states = outputs[0]
  sentence_embedding = torch.mean(last_hidden_states, dim=1)
  vanilla_test_embeddings.append(sentence_embedding[0].detach().cpu())

from transformers import RobertaModel, RobertaTokenizer

# Load pre-trained model and tokenizer
tokenizer = RobertaTokenizer.from_pretrained('roberta-base')
model = RobertaModel.from_pretrained('roberta-base')

plain_text_list = list(val_v1['tweet'])
vanilla_val_embeddings = []

# Generate embeddings for each text in the list
for text in plain_text_list:
  if not isinstance(text, str):
    text = str(text)
  text_input = tokenizer(text, return_tensors="pt", padding='longest', truncation=True)
  with torch.no_grad():
    outputs = model(**text_input)
  last_hidden_states = outputs[0]
  sentence_embedding = torch.mean(last_hidden_states, dim=1)
  vanilla_val_embeddings.append(sentence_embedding[0].detach().cpu())

"""#### Upload to Drive:"""

import pickle
with open("/content/drive/MyDrive/Contrastive NLP Stuff/Data/SOTA Data/Embeddings/IAC-V1/train_vanilla", "wb") as fp:
  pickle.dump(vanilla_train_embeddings, fp)

import pickle
with open("/content/drive/MyDrive/Contrastive NLP Stuff/Data/SOTA Data/Embeddings/IAC-V1/test_vanilla", "wb") as fp:
  pickle.dump(vanilla_test_embeddings, fp)

import pickle
with open("/content/drive/MyDrive/Contrastive NLP Stuff/Data/SOTA Data/Embeddings/IAC-V1/val_vanilla", "wb") as fp:
  pickle.dump(vanilla_val_embeddings, fp)

"""### Word Embeddings:"""

import nltk
nltk.download('stopwords')
nltk.download('punkt')

import re
import numpy as np
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from gensim.models import Word2Vec

stop_words = set(stopwords.words('english'))

def preprocess_text(text):
  text = text.lower()
  text = re.sub(r'[^\w\s]', '', text)
  tokens = word_tokenize(text)
  tokens = [token for token in tokens if token not in stop_words]
  return tokens

train_texts = [preprocess_text(text) for text in train_v1['tweet']]
test_texts = [preprocess_text(text) for text in test_v1['tweet']]
val_texts = [preprocess_text(text) for text in val_v1['tweet']]

model = Word2Vec(sentences=train_texts, vector_size=768, window=5, min_count=1, workers=4)

model.save("word2vec.model")

def text_to_embedding(text, model, max_len):
  embeddings = [model.wv[word] for word in text if word in model.wv]
  if len(embeddings) < max_len:
    embeddings = np.pad(embeddings, ((0, max_len - len(embeddings)), (0, 0)), mode='constant')
  else:
    embeddings = embeddings[:max_len]
  return embeddings

max_len = 50
train_embeddings = [text_to_embedding(text, model, max_len) for text in train_texts]
test_embeddings = [text_to_embedding(text, model, max_len) for text in test_texts]
val_embeddings = [text_to_embedding(text, model, max_len) for text in val_texts]

"""#### Upload to Drive:"""

import pickle
with open("/content/drive/MyDrive/Contrastive NLP Stuff/Data/SOTA Data/Embeddings/IAC-V1/train_word", "wb") as fp:
  pickle.dump(train_embeddings, fp)

import pickle
with open("/content/drive/MyDrive/Contrastive NLP Stuff/Data/SOTA Data/Embeddings/IAC-V1/test_word", "wb") as fp:
  pickle.dump(test_embeddings, fp)

import pickle
with open("/content/drive/MyDrive/Contrastive NLP Stuff/Data/SOTA Data/Embeddings/IAC-V1/val_word", "wb") as fp:
  pickle.dump(val_embeddings, fp)

"""## Tweets:

### Dataframe Formatting:
"""

column_names = ['id', 'sarcastic', 'tweet']
train_twitter = pd.read_csv('/content/drive/MyDrive/Contrastive NLP Stuff/Data/SOTA Data/CSV Data/Twitter/train.txt', sep='\t', names=column_names)
train_twitter = train_twitter.drop(columns=['id'])
train_twitter = train_twitter[['tweet', 'sarcastic']]
print(train_twitter.head())

test_twitter = pd.read_csv('/content/drive/MyDrive/Contrastive NLP Stuff/Data/SOTA Data/CSV Data/Twitter/test.txt', sep='\t', names=column_names)
test_twitter = test_twitter.drop(columns=['id'])
test_twitter = test_twitter[['tweet', 'sarcastic']]
print(test_twitter.head())

valid_twitter = pd.read_csv('/content/drive/MyDrive/Contrastive NLP Stuff/Data/SOTA Data/CSV Data/Twitter/valid.txt', sep='\t', names=column_names)
valid_twitter = valid_twitter.drop(columns=['id'])
valid_twitter = valid_twitter[['tweet', 'sarcastic']]
print(valid_twitter.head())

train_twitter.to_csv('train.csv')
test_twitter.to_csv('test.csv')
valid_twitter.to_csv('val.csv')

import shutil
shutil.copy("/content/train.csv","/content/drive/MyDrive/Contrastive NLP Stuff/Data/SOTA Data/CSV Data/Formatted Twitter")
shutil.copy("/content/test.csv","/content/drive/MyDrive/Contrastive NLP Stuff/Data/SOTA Data/CSV Data/Formatted Twitter")
shutil.copy("/content/val.csv","/content/drive/MyDrive/Contrastive NLP Stuff/Data/SOTA Data/CSV Data/Formatted Twitter")

"""### BERTweet SimCLR Sentence Embeddings:

#### BERTweet Model (Trained via SimCLR Framework):
"""

class config:
  batch_size = 50
  num_workers = 4
  text_encoder_lr = 1e-5
  weight_decay = 1e-3
  epochs = 10
  device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

  model_name = 'vinai/bertweet-base'
  text_embedding = 768
  max_length = 510
  embedding_size = 768
  hidden_size = 256

  pretrained = True
  trainable_language = True
  trainable_logic = True
  temperature = .7

  num_projection_layers = 2
  projection_dim = 256
  dropout = 0.01

from transformers import RobertaModel, RobertaTokenizer
# Used to encode the inputs using RoBERTa

class Encoder(torch.nn.Module):
  def __init__(self, model_name, trainable):
    super().__init__()
    self.model = RobertaModel.from_pretrained(model_name)
    for param in self.model.parameters():
      param.requires_grad = trainable

  def forward(self, input_ids, attention_mask):
    outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)
    last_hidden_state = outputs[0]
    pooled_output = last_hidden_state.mean(dim=1)
    return pooled_output

# Used to reduce the dimensionality of RoBERTa outputs and get it ready for classification task

class ProjectionHead(torch.nn.Module):
  def __init__(self, input_dim, output_dim, dropout_rate):
    super().__init__()

    self.projection = torch.nn.Sequential(
      torch.nn.Linear(input_dim, output_dim),
      torch.nn.ReLU(),
      torch.nn.Dropout(dropout_rate),
      torch.nn.Linear(output_dim, output_dim)
    )

  def forward(self, x):
    return self.projection(x)

from pytorch_metric_learning.losses import NTXentLoss
# Applies the SimCLR framework and creates a SimCLR model on the data

class SimCLR(nn.Module):
  def __init__(self, encoder, projection_head, temperature):
    super(SimCLR, self).__init__()
    self.encoder = encoder
    self.projection_head = projection_head
    self.temperature = temperature
    self.loss_fn = NTXentLoss(temperature)

  def forward(self, anchor_input, positive_input, negative_input):
    z_i = self.projection_head(self.encoder(**anchor_input))
    z_j = self.projection_head(self.encoder(**positive_input))
    z_k = self.projection_head(self.encoder(**negative_input))
    out = torch.cat([z_i, z_j], dim=0)
    labels = torch.cat([torch.arange(z_i.size(0)), torch.arange(z_i.size(0))], dim=0).to(z_i.device)

    return out, labels

  def get_loss(self, anchor_input, positive_input, negative_input):
    out, labels = self.forward(anchor_input, positive_input, negative_input)
    loss = self.loss_fn(out, labels)
    return loss

!gdown https://drive.google.com/uc?id=1NmF-x91w1uimC_cTOtykGJuw1SGZwo20
# Instantiate the SimCLR model
encoder = Encoder(config.model_name, config.trainable_language)
projection_head = ProjectionHead(config.text_embedding, config.projection_dim, config.dropout)
simclr_model = SimCLR(encoder, projection_head, config.temperature)
state_dict = torch.load("BERTweet_simclr_model_epoch_10.pth")
simclr_model.load_state_dict(state_dict)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

"""#### Generation:"""

!pip install emoji

simclr_model = simclr_model.to(config.device)
from transformers import AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained(config.model_name)

plain_text_list = list(train_twitter['tweet'])
train_embeddings = []

# Generate embeddings for each text in the list
for text in plain_text_list:
  if not isinstance(text, str):
    text = str(text)
  text_input = {key: val.to(config.device) for key, val in tokenizer(text, return_tensors="pt", padding='longest', truncation=True).items() if key != 'token_type_ids'}
  text_embeddings = simclr_model.encoder(**text_input)
  train_embeddings.append(text_embeddings[0].detach().cpu())

simclr_model = simclr_model.to(config.device)
from transformers import AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained(config.model_name)

plain_text_list = list(test_twitter['tweet'])
test_embeddings = []

# Generate embeddings for each text in the list
for text in plain_text_list:
  if not isinstance(text, str):
    text = str(text)
  text_input = {key: val.to(config.device) for key, val in tokenizer(text, return_tensors="pt", padding='longest', truncation=True).items() if key != 'token_type_ids'}
  text_embeddings = simclr_model.encoder(**text_input)
  test_embeddings.append(text_embeddings[0].detach().cpu())

simclr_model = simclr_model.to(config.device)
from transformers import AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained(config.model_name)

plain_text_list = list(valid_twitter['tweet'])
val_embeddings = []

# Generate embeddings for each text in the list
for text in plain_text_list:
  if not isinstance(text, str):
    text = str(text)
  text_input = {key: val.to(config.device) for key, val in tokenizer(text, return_tensors="pt", padding='longest', truncation=True).items() if key != 'token_type_ids'}
  text_embeddings = simclr_model.encoder(**text_input)
  val_embeddings.append(text_embeddings[0].detach().cpu())

"""#### Upload to Drive:"""

import pickle
with open("/content/drive/MyDrive/Contrastive NLP Stuff/Data/SOTA Data/Embeddings/Twitter/train_BERTweet_sentence", "wb") as fp:
  pickle.dump(train_embeddings, fp)

import pickle
with open("/content/drive/MyDrive/Contrastive NLP Stuff/Data/SOTA Data/Embeddings/Twitter/test_BERTweet_sentence", "wb") as fp:
  pickle.dump(test_embeddings, fp)

import pickle
with open("/content/drive/MyDrive/Contrastive NLP Stuff/Data/SOTA Data/Embeddings/Twitter/val_BERTweet_sentence", "wb") as fp:
  pickle.dump(val_embeddings, fp)

"""### BERTweet Sentence Embeddings:"""

!pip install emoji

from transformers import AutoModel, AutoTokenizer

# Load pre-trained model and tokenizer
MODEL = "vinai/bertweet-base"
tokenizer = AutoTokenizer.from_pretrained(MODEL)
model = AutoModel.from_pretrained(MODEL)

plain_text_list = list(train_twitter['tweet'])
train_embeddings = []

# Generate embeddings for each text in the list
for text in plain_text_list:
  if not isinstance(text, str):
    text = str(text)
  text_input = tokenizer(text, return_tensors="pt", padding='longest', truncation=True)
  with torch.no_grad():
    outputs = model(**text_input)
  last_hidden_states = outputs[0]
  sentence_embedding = torch.mean(last_hidden_states, dim=1)
  train_embeddings.append(sentence_embedding[0].detach().cpu())

from transformers import AutoModel, AutoTokenizer

# Load pre-trained model and tokenizer
MODEL = "vinai/bertweet-base"
tokenizer = AutoTokenizer.from_pretrained(MODEL)
model = AutoModel.from_pretrained(MODEL)

plain_text_list = list(test_twitter['tweet'])
test_embeddings = []

# Generate embeddings for each text in the list
for text in plain_text_list:
  if not isinstance(text, str):
    text = str(text)
  text_input = tokenizer(text, return_tensors="pt", padding='longest', truncation=True)
  with torch.no_grad():
    outputs = model(**text_input)
  last_hidden_states = outputs[0]
  sentence_embedding = torch.mean(last_hidden_states, dim=1)
  test_embeddings.append(sentence_embedding[0].detach().cpu())

from transformers import AutoModel, AutoTokenizer

# Load pre-trained model and tokenizer
MODEL = "vinai/bertweet-base"
tokenizer = AutoTokenizer.from_pretrained(MODEL)
model = AutoModel.from_pretrained(MODEL)

plain_text_list = list(valid_twitter['tweet'])
val_embeddings = []

# Generate embeddings for each text in the list
for text in plain_text_list:
  if not isinstance(text, str):
    text = str(text)
  text_input = tokenizer(text, return_tensors="pt", padding='longest', truncation=True)
  with torch.no_grad():
    outputs = model(**text_input)
  last_hidden_states = outputs[0]
  sentence_embedding = torch.mean(last_hidden_states, dim=1)
  val_embeddings.append(sentence_embedding[0].detach().cpu())

"""#### Upload to Drive:"""

import pickle
with open("/content/drive/MyDrive/Contrastive NLP Stuff/Data/SOTA Data/Embeddings/Twitter/BERTweet_train", "wb") as fp:
  pickle.dump(train_embeddings, fp)

import pickle
with open("/content/drive/MyDrive/Contrastive NLP Stuff/Data/SOTA Data/Embeddings/Twitter/BERTweet_test", "wb") as fp:
  pickle.dump(test_embeddings, fp)

import pickle
with open("/content/drive/MyDrive/Contrastive NLP Stuff/Data/SOTA Data/Embeddings/Twitter/BERTweet_val", "wb") as fp:
  pickle.dump(val_embeddings, fp)

"""### Vanilla Embeddings:"""

from transformers import RobertaModel, RobertaTokenizer

# Load pre-trained model and tokenizer
tokenizer = RobertaTokenizer.from_pretrained('roberta-base')
model = RobertaModel.from_pretrained('roberta-base')

plain_text_list = list(train_twitter['tweet'])
vanilla_train_embeddings = []

# Generate embeddings for each text in the list
for text in plain_text_list:
  if not isinstance(text, str):
    text = str(text)
  text_input = tokenizer(text, return_tensors="pt", padding='longest', truncation=True)
  with torch.no_grad():
    outputs = model(**text_input)
  last_hidden_states = outputs[0]
  sentence_embedding = torch.mean(last_hidden_states, dim=1)
  vanilla_train_embeddings.append(sentence_embedding[0].detach().cpu())

from transformers import RobertaModel, RobertaTokenizer

# Load pre-trained model and tokenizer
tokenizer = RobertaTokenizer.from_pretrained('roberta-base')
model = RobertaModel.from_pretrained('roberta-base')

plain_text_list = list(test_twitter['tweet'])
vanilla_test_embeddings = []

# Generate embeddings for each text in the list
for text in plain_text_list:
  if not isinstance(text, str):
    text = str(text)
  text_input = tokenizer(text, return_tensors="pt", padding='longest', truncation=True)
  with torch.no_grad():
    outputs = model(**text_input)
  last_hidden_states = outputs[0]
  sentence_embedding = torch.mean(last_hidden_states, dim=1)
  vanilla_test_embeddings.append(sentence_embedding[0].detach().cpu())

from transformers import RobertaModel, RobertaTokenizer

# Load pre-trained model and tokenizer
tokenizer = RobertaTokenizer.from_pretrained('roberta-base')
model = RobertaModel.from_pretrained('roberta-base')

plain_text_list = list(valid_twitter['tweet'])
vanilla_val_embeddings = []

# Generate embeddings for each text in the list
for text in plain_text_list:
  if not isinstance(text, str):
    text = str(text)
  text_input = tokenizer(text, return_tensors="pt", padding='longest', truncation=True)
  with torch.no_grad():
    outputs = model(**text_input)
  last_hidden_states = outputs[0]
  sentence_embedding = torch.mean(last_hidden_states, dim=1)
  vanilla_val_embeddings.append(sentence_embedding[0].detach().cpu())

"""#### Upload to Drive:"""

import pickle
with open("/content/drive/MyDrive/Contrastive NLP Stuff/Data/SOTA Data/Embeddings/Twitter/train_vanilla", "wb") as fp:
  pickle.dump(vanilla_train_embeddings, fp)

import pickle
with open("/content/drive/MyDrive/Contrastive NLP Stuff/Data/SOTA Data/Embeddings/Twitter/test_vanilla", "wb") as fp:
  pickle.dump(vanilla_test_embeddings, fp)

import pickle
with open("/content/drive/MyDrive/Contrastive NLP Stuff/Data/SOTA Data/Embeddings/Twitter/val_vanilla", "wb") as fp:
  pickle.dump(vanilla_val_embeddings, fp)

"""### Word Embeddings:"""

import nltk
nltk.download('stopwords')
nltk.download('punkt')

import re
import numpy as np
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from gensim.models import Word2Vec

stop_words = set(stopwords.words('english'))

def preprocess_text(text):
  text = text.lower()
  text = re.sub(r'[^\w\s]', '', text)
  tokens = word_tokenize(text)
  tokens = [token for token in tokens if token not in stop_words]
  return tokens

train_texts = [preprocess_text(text) for text in train_twitter['tweet']]
test_texts = [preprocess_text(text) for text in test_twitter['tweet']]
val_texts = [preprocess_text(text) for text in valid_twitter['tweet']]

model = Word2Vec(sentences=train_texts, vector_size=768, window=5, min_count=1, workers=4)

model.save("word2vec.model")

def text_to_embedding(text, model, max_len):
  embeddings = [model.wv[word] for word in text if word in model.wv]
  if not embeddings:
    return np.zeros((max_len, model.vector_size))
  elif len(embeddings) < max_len:
    return np.pad(embeddings, ((0, max_len - len(embeddings)), (0, 0)), mode='constant')
  else:
    return np.array(embeddings[:max_len])

max_len = 50
train_embeddings = [text_to_embedding(text, model, max_len) for text in train_texts]
test_embeddings = [text_to_embedding(text, model, max_len) for text in test_texts]
val_embeddings = [text_to_embedding(text, model, max_len) for text in val_texts]

"""#### Upload to Drive:"""

import pickle
with open("/content/drive/MyDrive/Contrastive NLP Stuff/Data/SOTA Data/Embeddings/Twitter/train_word", "wb") as fp:
  pickle.dump(train_embeddings, fp)

import pickle
with open("/content/drive/MyDrive/Contrastive NLP Stuff/Data/SOTA Data/Embeddings/Twitter/test_word", "wb") as fp:
  pickle.dump(test_embeddings, fp)

import pickle
with open("/content/drive/MyDrive/Contrastive NLP Stuff/Data/SOTA Data/Embeddings/Twitter/val_word", "wb") as fp:
  pickle.dump(val_embeddings, fp)