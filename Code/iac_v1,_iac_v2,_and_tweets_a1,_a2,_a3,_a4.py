# -*- coding: utf-8 -*-
"""IAC-V1, IAC-V2, and Tweets A1, A2, A3, A4

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14gG1f09zZaOGKmIxLGKnPLXBjgOzSfMu

## Imports:
"""

from google.colab import drive
drive.mount('/content/drive')

!pip install transformers
!pip install emoji
!pip install optuna
!pip install evaluate
!pip install -q bitsandbytes
!pip install pytorch_metric_learning
!pip install sentencepiece
!pip install rouge_score
!pip install accelerate -U
import pandas as pd
import numpy as np

import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.utils.data import DataLoader, TensorDataset

import transformers
from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModel

import bitsandbytes as bnb

from tqdm import tqdm
from operator import itemgetter
from pytorch_metric_learning.losses import NTXentLoss

"""## IAC-V1:

### A1:
"""

import pandas as pd
import numpy as np
import warnings
warnings.filterwarnings('ignore')
import torch
from torch import nn
from torch.utils.data import Dataset
from transformers import Trainer,TrainingArguments
from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score
from scipy import stats

class FFNNClassifier(nn.Module):
  def __init__(self, embed_dim, hidden_dim, num_classes):
    super(FFNNClassifier, self).__init__()
    self.fc1 = nn.Linear(embed_dim, hidden_dim)
    self.relu = nn.ReLU()
    self.fc2 = nn.Linear(hidden_dim, num_classes)

  def forward(self, embeddings, labels=None):
    batch_size, seq_len, embed_dim = embeddings.shape
    embeddings = embeddings.view(batch_size * seq_len, embed_dim)
    embeddings = embeddings.float()
    out = self.fc1(embeddings)
    out = self.relu(out)
    logits = self.fc2(out)
    logits = logits.view(batch_size, seq_len, -1)
    return {'logits': logits}

class SarcasmDataset(torch.utils.data.Dataset):
  def __init__(self, embeddings, labels):
    self.embeddings = embeddings
    self.labels = labels

  def __getitem__(self, idx):
    item = {'embeddings': self.embeddings[idx], 'labels': torch.tensor(self.labels[idx])}
    return item

  def __len__(self):
    return len(self.labels)

def compute_metrics(p):
  pred, labels = p
  aggregated_logits = stats.mode(pred, axis=1)[0]
  probabilities = np.exp(aggregated_logits) / np.sum(np.exp(aggregated_logits), axis=-1, keepdims=True)
  pred = np.argmax(probabilities, axis=-1)
  accuracy = accuracy_score(y_true=labels, y_pred=pred)
  f1 = f1_score(y_true=labels, y_pred=pred, average='binary')
  precision = precision_score(y_true=labels, y_pred=pred, average='binary')
  recall = recall_score(y_true=labels, y_pred=pred, average='binary')
  return {"accuracy": accuracy, "f1": f1, "precision": precision, "recall": recall}

class CustomTrainer(Trainer):
  def compute_loss(self, model, inputs, return_outputs=False):
    embeddings = inputs.get("embeddings")
    outputs = model(embeddings)
    logits = outputs['logits']
    labels = inputs.get("labels").to(logits.device)
    probabilities = torch.nn.functional.softmax(logits, dim=-1)
    sentence_probabilities = torch.mean(probabilities, dim=1)
    sentence_logits = torch.log(sentence_probabilities)
    loss_fct = nn.CrossEntropyLoss()
    loss = loss_fct(sentence_logits, labels)
    return (loss, outputs) if return_outputs else loss

import pickle
from transformers import AutoTokenizer, TrainingArguments, Trainer, AdamW
from torch import stack

# Load word-level embeddings
with open("/content/drive/MyDrive/Contrastive NLP Stuff/Data/SOTA Data/Embeddings/IAC-V1/train_word", "rb") as fp:
  train_embeddings = pickle.load(fp)

with open("/content/drive/MyDrive/Contrastive NLP Stuff/Data/SOTA Data/Embeddings/IAC-V1/test_word", "rb") as fp:
  test_embeddings = pickle.load(fp)

with open("/content/drive/MyDrive/Contrastive NLP Stuff/Data/SOTA Data/Embeddings/IAC-V1/val_word", "rb") as fp:
  val_embeddings = pickle.load(fp)

# Load data
train = pd.read_csv('/content/drive/MyDrive/Contrastive NLP Stuff/Data/SOTA Data/CSV Data/Formatted IAC-V1/train.csv')
test = pd.read_csv('/content/drive/MyDrive/Contrastive NLP Stuff/Data/SOTA Data/CSV Data/Formatted IAC-V1/test.csv')
val = pd.read_csv('/content/drive/MyDrive/Contrastive NLP Stuff/Data/SOTA Data/CSV Data/Formatted IAC-V1/val.csv')

train_embeddings = [embedding.astype(np.float32) for embedding in train_embeddings]
test_embeddings = [embedding.astype(np.float32) for embedding in test_embeddings]
val_embeddings = [embedding.astype(np.float32) for embedding in val_embeddings]

train_labels = train['sarcastic'].values.tolist()
test_labels = test['sarcastic'].values.tolist()
val_labels = val['sarcastic'].values.tolist()

train_dataset = SarcasmDataset(train_embeddings, train_labels)
test_dataset = SarcasmDataset(test_embeddings, test_labels)
val_dataset = SarcasmDataset(val_embeddings, val_labels)

embed_dim = 768
hidden_dim = 128
num_classes = 2

model = FFNNClassifier(embed_dim, hidden_dim, num_classes)

training_args = TrainingArguments(
  output_dir='./res', evaluation_strategy="steps", num_train_epochs=5, per_device_train_batch_size=32,
  per_device_eval_batch_size=64, warmup_steps=500, weight_decay=0.01,logging_dir='./logs4',
  load_best_model_at_end=True,
)

trainer = CustomTrainer(
  model=model, args=training_args, train_dataset=train_dataset,
  eval_dataset=val_dataset,
  compute_metrics=compute_metrics,
)

trainer.train()

# Evaluate on test dataset after training
trainer.evaluate(test_dataset)

"""### A2 RoBERTa:"""

import pandas as pd
import numpy as np
import warnings
warnings.filterwarnings('ignore')
import torch
from torch import nn
from torch.utils.data import Dataset
from transformers import Trainer,TrainingArguments
from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score

class FFNNClassifier(nn.Module):
  def __init__(self, embed_dim, hidden_dim, num_classes):
    super(FFNNClassifier, self).__init__()
    self.fc1 = nn.Linear(embed_dim, hidden_dim)
    self.relu = nn.ReLU()
    self.fc2 = nn.Linear(hidden_dim, num_classes)

  def forward(self, embeddings, labels=None):
    out = self.fc1(embeddings)
    out = self.relu(out)
    logits = self.fc2(out)
    return {'logits': logits}

class SarcasmDataset(torch.utils.data.Dataset):
  def __init__(self, embeddings, labels):
    self.embeddings = embeddings
    self.labels = labels

  def __getitem__(self, idx):
    item = {'embeddings': self.embeddings[idx], 'labels': torch.tensor(self.labels[idx])}
    return item

  def __len__(self):
    return len(self.labels)

def compute_metrics(p):
  pred, labels = p
  pred = np.argmax(pred, axis=1)

  accuracy = accuracy_score(y_true=labels, y_pred=pred)
  f1 = f1_score(labels, pred)
  precision = precision_score(labels, pred)
  recall = recall_score(labels, pred)

  return {"accuracy": accuracy, "f1_score": f1, "precision": precision, "recall": recall}

class CustomTrainer(Trainer):
  def compute_loss(self, model, inputs, return_outputs=False):
    embeddings = inputs.get("embeddings")
    outputs = model(embeddings)
    logits = outputs['logits']
    labels = inputs.get("labels").to(logits.device)
    loss_fct = nn.CrossEntropyLoss()
    loss = loss_fct(logits.view(-1, num_classes), labels.view(-1))
    return (loss, outputs) if return_outputs else loss

import pickle
from transformers import AutoTokenizer, TrainingArguments, Trainer, AdamW
from torch import stack

# Load sentence-level embeddings
with open("/content/drive/MyDrive/Contrastive NLP Stuff/Data/SOTA Data/Embeddings/IAC-V1/train_vanilla", "rb") as fp:
  vanilla_train = pickle.load(fp)

with open("/content/drive/MyDrive/Contrastive NLP Stuff/Data/SOTA Data/Embeddings/IAC-V1/test_vanilla", "rb") as fp:
  vanilla_test = pickle.load(fp)

with open("/content/drive/MyDrive/Contrastive NLP Stuff/Data/SOTA Data/Embeddings/IAC-V1/val_vanilla", "rb") as fp:
  vanilla_val = pickle.load(fp)

# Load data
train = pd.read_csv('/content/drive/MyDrive/Contrastive NLP Stuff/Data/SOTA Data/CSV Data/Formatted IAC-V1/train.csv')
test = pd.read_csv('/content/drive/MyDrive/Contrastive NLP Stuff/Data/SOTA Data/CSV Data/Formatted IAC-V1/test.csv')
val = pd.read_csv('/content/drive/MyDrive/Contrastive NLP Stuff/Data/SOTA Data/CSV Data/Formatted IAC-V1/val.csv')

train_labels = train['sarcastic'].values.tolist()
test_labels = test['sarcastic'].values.tolist()
val_labels = val['sarcastic'].values.tolist()

train_dataset = SarcasmDataset(vanilla_train, train_labels)
test_dataset = SarcasmDataset(vanilla_test, test_labels)
val_dataset = SarcasmDataset(vanilla_val, val_labels)

embed_dim = 768
hidden_dim = 128
num_classes = 2

model = FFNNClassifier(embed_dim, hidden_dim, num_classes)

training_args = TrainingArguments(
  output_dir='./res', evaluation_strategy="steps", num_train_epochs=5, per_device_train_batch_size=32,
  per_device_eval_batch_size=64, warmup_steps=500, weight_decay=0.01,logging_dir='./logs4',
  load_best_model_at_end=True,
)

trainer = CustomTrainer(
  model=model, args=training_args, train_dataset=train_dataset,
  eval_dataset=val_dataset,
  compute_metrics=compute_metrics,
)

trainer.train()

# Evaluate on test dataset after training
trainer.evaluate(test_dataset)

"""### A2 BERTweet:"""

import pandas as pd
import numpy as np
import warnings
warnings.filterwarnings('ignore')
import torch
from torch import nn
from torch.utils.data import Dataset
from transformers import Trainer,TrainingArguments
from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score

class FFNNClassifier(nn.Module):
  def __init__(self, embed_dim, hidden_dim, num_classes):
    super(FFNNClassifier, self).__init__()
    self.fc1 = nn.Linear(embed_dim, hidden_dim)
    self.relu = nn.ReLU()
    self.fc2 = nn.Linear(hidden_dim, num_classes)

  def forward(self, embeddings, labels=None):
    out = self.fc1(embeddings)
    out = self.relu(out)
    logits = self.fc2(out)
    return {'logits': logits}

class SarcasmDataset(torch.utils.data.Dataset):
  def __init__(self, embeddings, labels):
    self.embeddings = embeddings
    self.labels = labels

  def __getitem__(self, idx):
    item = {'embeddings': self.embeddings[idx], 'labels': torch.tensor(self.labels[idx])}
    return item

  def __len__(self):
    return len(self.labels)

def compute_metrics(p):
  pred, labels = p
  pred = np.argmax(pred, axis=1)

  accuracy = accuracy_score(y_true=labels, y_pred=pred)
  f1 = f1_score(labels, pred)
  precision = precision_score(labels, pred)
  recall = recall_score(labels, pred)

  return {"accuracy": accuracy, "f1_score": f1, "precision": precision, "recall": recall}

class CustomTrainer(Trainer):
  def compute_loss(self, model, inputs, return_outputs=False):
    embeddings = inputs.get("embeddings")
    outputs = model(embeddings)
    logits = outputs['logits']
    labels = inputs.get("labels").to(logits.device)
    loss_fct = nn.CrossEntropyLoss()
    loss = loss_fct(logits.view(-1, num_classes), labels.view(-1))
    return (loss, outputs) if return_outputs else loss

import pickle
from transformers import AutoTokenizer, TrainingArguments, Trainer, AdamW
from torch import stack

# Load sentence-level embeddings
with open("/content/drive/MyDrive/Contrastive NLP Stuff/Data/SOTA Data/Embeddings/IAC-V1/BERTweet_train", "rb") as fp:
  train_embeddings = pickle.load(fp)

with open("/content/drive/MyDrive/Contrastive NLP Stuff/Data/SOTA Data/Embeddings/IAC-V1/BERTweet_test", "rb") as fp:
  test_embeddings = pickle.load(fp)

with open("/content/drive/MyDrive/Contrastive NLP Stuff/Data/SOTA Data/Embeddings/IAC-V1/BERTweet_val", "rb") as fp:
  val_embeddings = pickle.load(fp)

# Load data
train = pd.read_csv('/content/drive/MyDrive/Contrastive NLP Stuff/Data/SOTA Data/CSV Data/Formatted IAC-V1/train.csv')
test = pd.read_csv('/content/drive/MyDrive/Contrastive NLP Stuff/Data/SOTA Data/CSV Data/Formatted IAC-V1/test.csv')
val = pd.read_csv('/content/drive/MyDrive/Contrastive NLP Stuff/Data/SOTA Data/CSV Data/Formatted IAC-V1/val.csv')

train_labels = train['sarcastic'].values.tolist()
test_labels = test['sarcastic'].values.tolist()
val_labels = val['sarcastic'].values.tolist()

train_dataset = SarcasmDataset(train_embeddings, train_labels)
test_dataset = SarcasmDataset(test_embeddings, test_labels)
val_dataset = SarcasmDataset(val_embeddings, val_labels)

embed_dim = 768
hidden_dim = 128
num_classes = 2

model = FFNNClassifier(embed_dim, hidden_dim, num_classes)

training_args = TrainingArguments(
  output_dir='./res', evaluation_strategy="steps", num_train_epochs=5, per_device_train_batch_size=32,
  per_device_eval_batch_size=64, warmup_steps=500, weight_decay=0.01,logging_dir='./logs4',
  load_best_model_at_end=True,
)

trainer = CustomTrainer(
  model=model, args=training_args, train_dataset=train_dataset,
  eval_dataset=val_dataset,
  compute_metrics=compute_metrics,
)

trainer.train()

trainer.evaluate(test_dataset)

"""### A3:"""

import pandas as pd
import numpy as np
import warnings
warnings.filterwarnings('ignore')
import torch
from torch import nn
from torch.utils.data import Dataset
from transformers import Trainer,TrainingArguments
from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score

class FFNNClassifier(nn.Module):
  def __init__(self, embed_dim, hidden_dim, num_classes):
    super(FFNNClassifier, self).__init__()
    self.fc1 = nn.Linear(embed_dim, hidden_dim)
    self.relu = nn.ReLU()
    self.fc2 = nn.Linear(hidden_dim, num_classes)

  def forward(self, embeddings, labels=None):
    out = self.fc1(embeddings)
    out = self.relu(out)
    logits = self.fc2(out)
    return {'logits': logits}

class SarcasmDataset(torch.utils.data.Dataset):
  def __init__(self, embeddings, labels):
    self.embeddings = embeddings
    self.labels = labels

  def __getitem__(self, idx):
    item = {'embeddings': self.embeddings[idx], 'labels': torch.tensor(self.labels[idx])}
    return item

  def __len__(self):
    return len(self.labels)

def compute_metrics(p):
  pred, labels = p
  pred = np.argmax(pred, axis=1)

  accuracy = accuracy_score(y_true=labels, y_pred=pred)
  f1 = f1_score(labels, pred)
  precision = precision_score(labels, pred)
  recall = recall_score(labels, pred)

  return {"accuracy": accuracy, "f1_score": f1, "precision": precision, "recall": recall}

class CustomTrainer(Trainer):
  def compute_loss(self, model, inputs, return_outputs=False):
    embeddings = inputs.get("embeddings")
    outputs = model(embeddings)
    logits = outputs['logits']
    labels = inputs.get("labels").to(logits.device)
    loss_fct = nn.CrossEntropyLoss()
    loss = loss_fct(logits.view(-1, num_classes), labels.view(-1))
    return (loss, outputs) if return_outputs else loss

import pickle
from transformers import AutoTokenizer, TrainingArguments, Trainer, AdamW
from torch import stack

# Load sentence-level embeddings
with open("/content/drive/MyDrive/Contrastive NLP Stuff/Data/SOTA Data/Embeddings/IAC-V1/train_BERTweet_sentence", "rb") as fp:
  train_embeddings = pickle.load(fp)

with open("/content/drive/MyDrive/Contrastive NLP Stuff/Data/SOTA Data/Embeddings/IAC-V1/test_BERTweet_sentence", "rb") as fp:
  test_embeddings = pickle.load(fp)

with open("/content/drive/MyDrive/Contrastive NLP Stuff/Data/SOTA Data/Embeddings/IAC-V1/val_BERTweet_sentence", "rb") as fp:
  val_embeddings = pickle.load(fp)

# Load data
train = pd.read_csv('/content/drive/MyDrive/Contrastive NLP Stuff/Data/SOTA Data/CSV Data/Formatted IAC-V1/train.csv')
test = pd.read_csv('/content/drive/MyDrive/Contrastive NLP Stuff/Data/SOTA Data/CSV Data/Formatted IAC-V1/test.csv')
val = pd.read_csv('/content/drive/MyDrive/Contrastive NLP Stuff/Data/SOTA Data/CSV Data/Formatted IAC-V1/val.csv')

train_labels = train['sarcastic'].values.tolist()
test_labels = test['sarcastic'].values.tolist()
val_labels = val['sarcastic'].values.tolist()

train_dataset = SarcasmDataset(train_embeddings, train_labels)
test_dataset = SarcasmDataset(test_embeddings, test_labels)
val_dataset = SarcasmDataset(val_embeddings, val_labels)

embed_dim = 768
hidden_dim = 128
num_classes = 2

model = FFNNClassifier(embed_dim, hidden_dim, num_classes)

training_args = TrainingArguments(
  output_dir='./res', evaluation_strategy="steps", num_train_epochs=5, per_device_train_batch_size=32,
  per_device_eval_batch_size=64, warmup_steps=500, weight_decay=0.01,logging_dir='./logs4',
  load_best_model_at_end=True,
)

trainer = CustomTrainer(
  model=model, args=training_args, train_dataset=train_dataset,
  eval_dataset=val_dataset,
  compute_metrics=compute_metrics,
)

trainer.train()

# Evaluate on test dataset after training
trainer.evaluate(test_dataset)

"""### A4:"""

import torch
from torch import nn
from torch.utils.data import Dataset
from transformers import Trainer, TrainingArguments
from sklearn.metrics import accuracy_score, precision_recall_fscore_support
import numpy as np
import pickle
from transformers import AdamW

class FFNNClassifier(nn.Module):
  def __init__(self, word_embed_dim, sentence_embed_dim, hidden_dim):
    super(FFNNClassifier, self).__init__()
    self.lstm = nn.LSTM(input_size=word_embed_dim, hidden_size=hidden_dim, batch_first=True)
    self.sentence_fc = nn.Linear(sentence_embed_dim, hidden_dim)
    self.fc = nn.Linear(hidden_dim * 2, hidden_dim)
    self.relu = nn.ReLU()
    self.classifier = nn.Linear(hidden_dim, 1)

  def forward(self, word_embeddings, simclr_embeddings, vanilla_embeddings, bertweet_embeddings, labels=None):
    _, (hidden, _) = self.lstm(word_embeddings)
    word_repr = hidden[-1]

    combined_sentence_embeddings = torch.cat([simclr_embeddings, vanilla_embeddings, bertweet_embeddings], dim=1)
    sentence_repr = self.relu(self.sentence_fc(combined_sentence_embeddings))

    combined = torch.cat([word_repr, sentence_repr], dim=1)
    combined = self.relu(self.fc(combined))

    logits = self.classifier(combined).squeeze(-1)

    loss = None
    if labels is not None:
      loss_fct = nn.BCEWithLogitsLoss()
      loss = loss_fct(logits.view(-1), labels.view(-1))

    return (loss, logits) if loss is not None else logits

class SarcasmDataset(Dataset):
  def __init__(self, labels, word_embeddings, simclr_embeddings, vanilla_embeddings, bertweet_embeddings):
    self.labels = labels
    self.word_embeddings = word_embeddings
    self.simclr_embeddings = simclr_embeddings
    self.vanilla_embeddings = vanilla_embeddings
    self.bertweet_embeddings = bertweet_embeddings

  def __getitem__(self, idx):
    return {
      'word_embeddings': torch.tensor(self.word_embeddings[idx], dtype=torch.float),
      'simclr_embeddings': self.simclr_embeddings[idx].clone().detach(),
      'vanilla_embeddings': self.vanilla_embeddings[idx].clone().detach(),
      'bertweet_embeddings': self.bertweet_embeddings[idx].clone().detach(),
      'labels': torch.tensor(self.labels[idx], dtype=torch.float)
    }

  def __len__(self):
    return len(self.labels)

def compute_metrics(eval_pred):
  logits, labels = eval_pred
  preds = torch.sigmoid(torch.tensor(logits)).round().numpy()
  precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')
  acc = accuracy_score(labels, preds)
  return {
    'accuracy': acc,
    'f1': f1,
    'precision': precision,
    'recall': recall
  }

with open("/content/drive/MyDrive/Contrastive NLP Stuff/Data/SOTA Data/Embeddings/IAC-V1/train_BERTweet_sentence", "rb") as fp:
  train_simclr_embeddings = pickle.load(fp)

with open("/content/drive/MyDrive/Contrastive NLP Stuff/Data/SOTA Data/Embeddings/IAC-V1/test_BERTweet_sentence", "rb") as fp:
  test_simclr_embeddings = pickle.load(fp)

with open("/content/drive/MyDrive/Contrastive NLP Stuff/Data/SOTA Data/Embeddings/IAC-V1/val_BERTweet_sentence", "rb") as fp:
  val_simclr_embeddings = pickle.load(fp)

with open("/content/drive/MyDrive/Contrastive NLP Stuff/Data/SOTA Data/Embeddings/IAC-V1/train_vanilla", "rb") as fp:
  vanilla_train = pickle.load(fp)

with open("/content/drive/MyDrive/Contrastive NLP Stuff/Data/SOTA Data/Embeddings/IAC-V1/test_vanilla", "rb") as fp:
  vanilla_test = pickle.load(fp)

with open("/content/drive/MyDrive/Contrastive NLP Stuff/Data/SOTA Data/Embeddings/IAC-V1/val_vanilla", "rb") as fp:
  vanilla_val = pickle.load(fp)

with open("/content/drive/MyDrive/Contrastive NLP Stuff/Data/SOTA Data/Embeddings/IAC-V1/train_word", "rb") as fp:
  train_word_embeddings = pickle.load(fp)

with open("/content/drive/MyDrive/Contrastive NLP Stuff/Data/SOTA Data/Embeddings/IAC-V1/test_word", "rb") as fp:
  test_word_embeddings = pickle.load(fp)

with open("/content/drive/MyDrive/Contrastive NLP Stuff/Data/SOTA Data/Embeddings/IAC-V1/val_word", "rb") as fp:
  val_word_embeddings = pickle.load(fp)

with open("/content/drive/MyDrive/Contrastive NLP Stuff/Data/SOTA Data/Embeddings/IAC-V1/BERTweet_train", "rb") as fp:
  train_bertweet_embeddings = pickle.load(fp)

with open("/content/drive/MyDrive/Contrastive NLP Stuff/Data/SOTA Data/Embeddings/IAC-V1/BERTweet_test", "rb") as fp:
  test_bertweet_embeddings = pickle.load(fp)

with open("/content/drive/MyDrive/Contrastive NLP Stuff/Data/SOTA Data/Embeddings/IAC-V1/BERTweet_val", "rb") as fp:
  val_bertweet_embeddings = pickle.load(fp)

train = pd.read_csv('/content/drive/MyDrive/Contrastive NLP Stuff/Data/SOTA Data/CSV Data/Formatted IAC-V1/train.csv')
test = pd.read_csv('/content/drive/MyDrive/Contrastive NLP Stuff/Data/SOTA Data/CSV Data/Formatted IAC-V1/test.csv')
val = pd.read_csv('/content/drive/MyDrive/Contrastive NLP Stuff/Data/SOTA Data/CSV Data/Formatted IAC-V1/val.csv')

train_labels = train['sarcastic'].values.tolist()
test_labels = test['sarcastic'].values.tolist()
val_labels = val['sarcastic'].values.tolist()

train_dataset = SarcasmDataset(train_labels, train_word_embeddings, train_simclr_embeddings, vanilla_train, train_bertweet_embeddings)
test_dataset = SarcasmDataset(test_labels, test_word_embeddings, test_simclr_embeddings, vanilla_test, test_bertweet_embeddings)
val_dataset = SarcasmDataset(val_labels, val_word_embeddings, val_simclr_embeddings, vanilla_val, val_bertweet_embeddings)

model = FFNNClassifier(word_embed_dim=768, sentence_embed_dim=768*3, hidden_dim=1024)

optimizer = AdamW(model.parameters(), lr=5e-5)

training_args = TrainingArguments(
  output_dir='./results',
  num_train_epochs=10,
  per_device_train_batch_size=16,
  per_device_eval_batch_size=16,
  warmup_steps=500,
  weight_decay=0.01,
  logging_dir='./logs',
  evaluation_strategy="epoch",
  save_strategy="epoch",
  load_best_model_at_end=True,
)

trainer = Trainer(
  model=model,
  args=training_args,
  train_dataset=train_dataset,
  eval_dataset=val_dataset,
  compute_metrics=compute_metrics,
  optimizers=(optimizer, None),
)

trainer.train()
trainer.evaluate(test_dataset)

"""## Twitter:

### A1:
"""

import pandas as pd
import numpy as np
import warnings
warnings.filterwarnings('ignore')
import torch
from torch import nn
from torch.utils.data import Dataset
from transformers import Trainer,TrainingArguments
from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score
from scipy import stats

class FFNNClassifier(nn.Module):
  def __init__(self, embed_dim, hidden_dim, num_classes):
    super(FFNNClassifier, self).__init__()
    self.fc1 = nn.Linear(embed_dim, hidden_dim)
    self.relu = nn.ReLU()
    self.fc2 = nn.Linear(hidden_dim, num_classes)

  def forward(self, embeddings, labels=None):
    batch_size, seq_len, embed_dim = embeddings.shape
    embeddings = embeddings.view(batch_size * seq_len, embed_dim)
    embeddings = embeddings.float()
    out = self.fc1(embeddings)
    out = self.relu(out)
    logits = self.fc2(out)
    logits = logits.view(batch_size, seq_len, -1)
    return {'logits': logits}

class SarcasmDataset(torch.utils.data.Dataset):
  def __init__(self, embeddings, labels):
    self.embeddings = embeddings
    self.labels = labels

  def __getitem__(self, idx):
    item = {'embeddings': self.embeddings[idx], 'labels': torch.tensor(self.labels[idx])}
    return item

  def __len__(self):
    return len(self.labels)

def compute_metrics(p):
  pred, labels = p
  aggregated_logits = stats.mode(pred, axis=1)[0]
  probabilities = np.exp(aggregated_logits) / np.sum(np.exp(aggregated_logits), axis=-1, keepdims=True)
  pred = np.argmax(probabilities, axis=-1)
  accuracy = accuracy_score(y_true=labels, y_pred=pred)
  f1 = f1_score(y_true=labels, y_pred=pred, average='binary')
  precision = precision_score(y_true=labels, y_pred=pred, average='binary')
  recall = recall_score(y_true=labels, y_pred=pred, average='binary')
  return {"accuracy": accuracy, "f1": f1, "precision": precision, "recall": recall}

class CustomTrainer(Trainer):
  def compute_loss(self, model, inputs, return_outputs=False):
    embeddings = inputs.get("embeddings")
    outputs = model(embeddings)
    logits = outputs['logits']
    labels = inputs.get("labels").to(logits.device)
    probabilities = torch.nn.functional.softmax(logits, dim=-1)
    sentence_probabilities = torch.mean(probabilities, dim=1)
    sentence_logits = torch.log(sentence_probabilities)
    loss_fct = nn.CrossEntropyLoss()
    loss = loss_fct(sentence_logits, labels)
    return (loss, outputs) if return_outputs else loss

import pickle
from transformers import AutoTokenizer, TrainingArguments, Trainer, AdamW
from torch import stack

# Load sentence-level embeddings
with open("/content/drive/MyDrive/Contrastive NLP Stuff/Data/SOTA Data/Embeddings/Twitter/train_word", "rb") as fp:
  train_embeddings = pickle.load(fp)

with open("/content/drive/MyDrive/Contrastive NLP Stuff/Data/SOTA Data/Embeddings/Twitter/test_word", "rb") as fp:
  test_embeddings = pickle.load(fp)

with open("/content/drive/MyDrive/Contrastive NLP Stuff/Data/SOTA Data/Embeddings/Twitter/val_word", "rb") as fp:
  val_embeddings = pickle.load(fp)

# Load data
train = pd.read_csv('/content/drive/MyDrive/Contrastive NLP Stuff/Data/SOTA Data/CSV Data/Formatted Twitter/train.csv')
test = pd.read_csv('/content/drive/MyDrive/Contrastive NLP Stuff/Data/SOTA Data/CSV Data/Formatted Twitter/test.csv')
val = pd.read_csv('/content/drive/MyDrive/Contrastive NLP Stuff/Data/SOTA Data/CSV Data/Formatted Twitter/val.csv')

train_labels = train['sarcastic'].values.tolist()
test_labels = test['sarcastic'].values.tolist()
val_labels = val['sarcastic'].values.tolist()

train_dataset = SarcasmDataset(train_embeddings, train_labels)
test_dataset = SarcasmDataset(test_embeddings, test_labels)
val_dataset = SarcasmDataset(val_embeddings, val_labels)

embed_dim = 768
hidden_dim = 128
num_classes = 2

model = FFNNClassifier(embed_dim, hidden_dim, num_classes)

training_args = TrainingArguments(
  output_dir='./res', evaluation_strategy="steps", num_train_epochs=5, per_device_train_batch_size=32,
  per_device_eval_batch_size=64, warmup_steps=500, weight_decay=0.01,logging_dir='./logs4',
  load_best_model_at_end=True,
)

trainer = CustomTrainer(
  model=model, args=training_args, train_dataset=train_dataset,
  eval_dataset=val_dataset,
  compute_metrics=compute_metrics,
)

trainer.train()

# Evaluate on test dataset after training
trainer.evaluate(test_dataset)

"""### A2 RoBERTa:"""

import pandas as pd
import numpy as np
import warnings
warnings.filterwarnings('ignore')
import torch
from torch import nn
from torch.utils.data import Dataset
from transformers import Trainer,TrainingArguments
from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score

class FFNNClassifier(nn.Module):
  def __init__(self, embed_dim, hidden_dim, num_classes):
    super(FFNNClassifier, self).__init__()
    self.fc1 = nn.Linear(embed_dim, hidden_dim)
    self.relu = nn.ReLU()
    self.fc2 = nn.Linear(hidden_dim, num_classes)

  def forward(self, embeddings, labels=None):
    out = self.fc1(embeddings)
    out = self.relu(out)
    logits = self.fc2(out)
    return {'logits': logits}

class SarcasmDataset(torch.utils.data.Dataset):
  def __init__(self, embeddings, labels):
    self.embeddings = embeddings
    self.labels = labels

  def __getitem__(self, idx):
    item = {'embeddings': self.embeddings[idx], 'labels': torch.tensor(self.labels[idx])}
    return item

  def __len__(self):
    return len(self.labels)

def compute_metrics(p):
  pred, labels = p
  pred = np.argmax(pred, axis=1)

  accuracy = accuracy_score(y_true=labels, y_pred=pred)
  f1 = f1_score(labels, pred)
  precision = precision_score(labels, pred)
  recall = recall_score(labels, pred)

  return {"accuracy": accuracy, "f1_score": f1, "precision": precision, "recall": recall}

class CustomTrainer(Trainer):
  def compute_loss(self, model, inputs, return_outputs=False):
    embeddings = inputs.get("embeddings")
    outputs = model(embeddings)
    logits = outputs['logits']
    labels = inputs.get("labels").to(logits.device)
    loss_fct = nn.CrossEntropyLoss()
    loss = loss_fct(logits.view(-1, num_classes), labels.view(-1))
    return (loss, outputs) if return_outputs else loss

import pickle
from transformers import AutoTokenizer, TrainingArguments, Trainer, AdamW
from torch import stack

# Load sentence-level embeddings
with open("/content/drive/MyDrive/Contrastive NLP Stuff/Data/SOTA Data/Embeddings/Twitter/train_vanilla", "rb") as fp:
  vanilla_train = pickle.load(fp)

with open("/content/drive/MyDrive/Contrastive NLP Stuff/Data/SOTA Data/Embeddings/Twitter/test_vanilla", "rb") as fp:
  vanilla_test = pickle.load(fp)

with open("/content/drive/MyDrive/Contrastive NLP Stuff/Data/SOTA Data/Embeddings/Twitter/train_vanilla", "rb") as fp:
  vanilla_val = pickle.load(fp)

# Load data
train = pd.read_csv('/content/drive/MyDrive/Contrastive NLP Stuff/Data/SOTA Data/CSV Data/Formatted Twitter/train.csv')
test = pd.read_csv('/content/drive/MyDrive/Contrastive NLP Stuff/Data/SOTA Data/CSV Data/Formatted Twitter/test.csv')
val = pd.read_csv('/content/drive/MyDrive/Contrastive NLP Stuff/Data/SOTA Data/CSV Data/Formatted Twitter/val.csv')

train_labels = train['sarcastic'].values.tolist()
test_labels = test['sarcastic'].values.tolist()
val_labels = val['sarcastic'].values.tolist()

train_dataset = SarcasmDataset(vanilla_train, train_labels)
test_dataset = SarcasmDataset(vanilla_test, test_labels)
val_dataset = SarcasmDataset(vanilla_val, val_labels)

embed_dim = 768
hidden_dim = 128
num_classes = 2

model = FFNNClassifier(embed_dim, hidden_dim, num_classes)

training_args = TrainingArguments(
  output_dir='./res', evaluation_strategy="steps", num_train_epochs=5, per_device_train_batch_size=32,
  per_device_eval_batch_size=64, warmup_steps=500, weight_decay=0.01,logging_dir='./logs4',
  load_best_model_at_end=True,
)

trainer = CustomTrainer(
  model=model, args=training_args, train_dataset=train_dataset,
  eval_dataset=val_dataset,
  compute_metrics=compute_metrics,
)

trainer.train()

# Evaluate on test dataset after training
trainer.evaluate(test_dataset)

"""### A2 BERTweet:"""

import pandas as pd
import numpy as np
import warnings
warnings.filterwarnings('ignore')
import torch
from torch import nn
from torch.utils.data import Dataset
from transformers import Trainer,TrainingArguments
from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score

class FFNNClassifier(nn.Module):
  def __init__(self, embed_dim, hidden_dim, num_classes):
    super(FFNNClassifier, self).__init__()
    self.fc1 = nn.Linear(embed_dim, hidden_dim)
    self.relu = nn.ReLU()
    self.fc2 = nn.Linear(hidden_dim, num_classes)

  def forward(self, embeddings, labels=None):
    out = self.fc1(embeddings)
    out = self.relu(out)
    logits = self.fc2(out)
    return {'logits': logits}

class SarcasmDataset(torch.utils.data.Dataset):
  def __init__(self, embeddings, labels):
    self.embeddings = embeddings
    self.labels = labels

  def __getitem__(self, idx):
    item = {'embeddings': self.embeddings[idx], 'labels': torch.tensor(self.labels[idx])}
    return item

  def __len__(self):
    return len(self.labels)

def compute_metrics(p):
  pred, labels = p
  pred = np.argmax(pred, axis=1)

  accuracy = accuracy_score(y_true=labels, y_pred=pred)
  f1 = f1_score(labels, pred)
  precision = precision_score(labels, pred)
  recall = recall_score(labels, pred)

  return {"accuracy": accuracy, "f1_score": f1, "precision": precision, "recall": recall}

class CustomTrainer(Trainer):
  def compute_loss(self, model, inputs, return_outputs=False):
    embeddings = inputs.get("embeddings")
    outputs = model(embeddings)
    logits = outputs['logits']
    labels = inputs.get("labels").to(logits.device)
    loss_fct = nn.CrossEntropyLoss()
    loss = loss_fct(logits.view(-1, num_classes), labels.view(-1))
    return (loss, outputs) if return_outputs else loss

import pickle
from transformers import AutoTokenizer, TrainingArguments, Trainer, AdamW
from torch import stack

# Load sentence-level embeddings
with open("/content/drive/MyDrive/Contrastive NLP Stuff/Data/SOTA Data/Embeddings/Twitter/BERTweet_train", "rb") as fp:
  train_embeddings = pickle.load(fp)

with open("/content/drive/MyDrive/Contrastive NLP Stuff/Data/SOTA Data/Embeddings/Twitter/BERTweet_test", "rb") as fp:
  test_embeddings = pickle.load(fp)

with open("/content/drive/MyDrive/Contrastive NLP Stuff/Data/SOTA Data/Embeddings/Twitter/BERTweet_val", "rb") as fp:
  val_embeddings = pickle.load(fp)

# Load data
train = pd.read_csv('/content/drive/MyDrive/Contrastive NLP Stuff/Data/SOTA Data/CSV Data/Formatted Twitter/train.csv')
test = pd.read_csv('/content/drive/MyDrive/Contrastive NLP Stuff/Data/SOTA Data/CSV Data/Formatted Twitter/test.csv')
val = pd.read_csv('/content/drive/MyDrive/Contrastive NLP Stuff/Data/SOTA Data/CSV Data/Formatted Twitter/val.csv')

train_labels = train['sarcastic'].values.tolist()
test_labels = test['sarcastic'].values.tolist()
val_labels = val['sarcastic'].values.tolist()

train_dataset = SarcasmDataset(train_embeddings, train_labels)
test_dataset = SarcasmDataset(test_embeddings, test_labels)
val_dataset = SarcasmDataset(val_embeddings, val_labels)

embed_dim = 768
hidden_dim = 128
num_classes = 2

model = FFNNClassifier(embed_dim, hidden_dim, num_classes)

training_args = TrainingArguments(
  output_dir='./res', evaluation_strategy="steps", num_train_epochs=5, per_device_train_batch_size=32,
  per_device_eval_batch_size=64, warmup_steps=500, weight_decay=0.01,logging_dir='./logs4',
  load_best_model_at_end=True,
)

trainer = CustomTrainer(
  model=model, args=training_args, train_dataset=train_dataset,
  eval_dataset=val_dataset,
  compute_metrics=compute_metrics,
)

trainer.train()

# Evaluate on test dataset after training
trainer.evaluate(test_dataset)

"""### A3:"""

import pandas as pd
import numpy as np
import warnings
warnings.filterwarnings('ignore')
import torch
from torch import nn
from torch.utils.data import Dataset
from transformers import Trainer,TrainingArguments
from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score

class FFNNClassifier(nn.Module):
  def __init__(self, embed_dim, hidden_dim, num_classes):
    super(FFNNClassifier, self).__init__()
    self.fc1 = nn.Linear(embed_dim, hidden_dim)
    self.relu = nn.ReLU()
    self.fc2 = nn.Linear(hidden_dim, num_classes)

  def forward(self, embeddings, labels=None):
    out = self.fc1(embeddings)
    out = self.relu(out)
    logits = self.fc2(out)
    return {'logits': logits}

class SarcasmDataset(torch.utils.data.Dataset):
  def __init__(self, embeddings, labels):
    self.embeddings = embeddings
    self.labels = labels

  def __getitem__(self, idx):
    item = {'embeddings': self.embeddings[idx], 'labels': torch.tensor(self.labels[idx])}
    return item

  def __len__(self):
    return len(self.labels)

def compute_metrics(p):
  pred, labels = p
  pred = np.argmax(pred, axis=1)

  accuracy = accuracy_score(y_true=labels, y_pred=pred)
  f1 = f1_score(labels, pred)
  precision = precision_score(labels, pred)
  recall = recall_score(labels, pred)

  return {"accuracy": accuracy, "f1_score": f1, "precision": precision, "recall": recall}

class CustomTrainer(Trainer):
  def compute_loss(self, model, inputs, return_outputs=False):
    embeddings = inputs.get("embeddings")
    outputs = model(embeddings)
    logits = outputs['logits']
    labels = inputs.get("labels").to(logits.device)
    loss_fct = nn.CrossEntropyLoss()
    loss = loss_fct(logits.view(-1, num_classes), labels.view(-1))
    return (loss, outputs) if return_outputs else loss

import pickle
from transformers import AutoTokenizer, TrainingArguments, Trainer, AdamW
from torch import stack

# Load sentence-level embeddings
with open("/content/drive/MyDrive/Contrastive NLP Stuff/Data/SOTA Data/Embeddings/Twitter/train_BERTweet_sentence", "rb") as fp:
  train_embeddings = pickle.load(fp)

with open("/content/drive/MyDrive/Contrastive NLP Stuff/Data/SOTA Data/Embeddings/Twitter/test_BERTweet_sentence", "rb") as fp:
  test_embeddings = pickle.load(fp)

with open("/content/drive/MyDrive/Contrastive NLP Stuff/Data/SOTA Data/Embeddings/Twitter/val_BERTweet_sentence", "rb") as fp:
  val_embeddings = pickle.load(fp)

# Load data
train = pd.read_csv('/content/drive/MyDrive/Contrastive NLP Stuff/Data/SOTA Data/CSV Data/Formatted Twitter/train.csv')
test = pd.read_csv('/content/drive/MyDrive/Contrastive NLP Stuff/Data/SOTA Data/CSV Data/Formatted Twitter/test.csv')
val = pd.read_csv('/content/drive/MyDrive/Contrastive NLP Stuff/Data/SOTA Data/CSV Data/Formatted Twitter/val.csv')

train_labels = train['sarcastic'].values.tolist()
test_labels = test['sarcastic'].values.tolist()
val_labels = val['sarcastic'].values.tolist()

train_dataset = SarcasmDataset(train_embeddings, train_labels)
test_dataset = SarcasmDataset(test_embeddings, test_labels)
val_dataset = SarcasmDataset(val_embeddings, val_labels)

embed_dim = 768
hidden_dim = 128
num_classes = 2

model = FFNNClassifier(embed_dim, hidden_dim, num_classes)

training_args = TrainingArguments(
  output_dir='./res', evaluation_strategy="steps", num_train_epochs=5, per_device_train_batch_size=32,
  per_device_eval_batch_size=64, warmup_steps=500, weight_decay=0.01,logging_dir='./logs4',
  load_best_model_at_end=True,
)

trainer = CustomTrainer(
  model=model, args=training_args, train_dataset=train_dataset,
  eval_dataset=val_dataset,
  compute_metrics=compute_metrics,
)

trainer.train()

# Evaluate on test dataset after training
trainer.evaluate(test_dataset)

"""### A4:"""

import torch
from torch import nn
from torch.utils.data import Dataset
from transformers import Trainer, TrainingArguments
from sklearn.metrics import accuracy_score, precision_recall_fscore_support
import numpy as np
import pickle
from transformers import AdamW

class FFNNClassifier(nn.Module):
  def __init__(self, word_embed_dim, sentence_embed_dim, hidden_dim):
    super(FFNNClassifier, self).__init__()
    self.lstm = nn.LSTM(input_size=word_embed_dim, hidden_size=hidden_dim, batch_first=True)
    self.sentence_fc = nn.Linear(sentence_embed_dim, hidden_dim)
    self.fc = nn.Linear(hidden_dim * 2, hidden_dim)
    self.relu = nn.ReLU()
    self.classifier = nn.Linear(hidden_dim, 1)

  def forward(self, word_embeddings, simclr_embeddings, vanilla_embeddings, bertweet_embeddings, labels=None):
    _, (hidden, _) = self.lstm(word_embeddings)
    word_repr = hidden[-1]

    combined_sentence_embeddings = torch.cat([simclr_embeddings, vanilla_embeddings, bertweet_embeddings], dim=1)
    sentence_repr = self.relu(self.sentence_fc(combined_sentence_embeddings))

    combined = torch.cat([word_repr, sentence_repr], dim=1)
    combined = self.relu(self.fc(combined))

    logits = self.classifier(combined).squeeze(-1)

    loss = None
    if labels is not None:
      loss_fct = nn.BCEWithLogitsLoss()
      loss = loss_fct(logits.view(-1), labels.view(-1))

    return (loss, logits) if loss is not None else logits

class SarcasmDataset(Dataset):
  def __init__(self, labels, word_embeddings, simclr_embeddings, vanilla_embeddings, bertweet_embeddings):
    self.labels = labels
    self.word_embeddings = word_embeddings
    self.simclr_embeddings = simclr_embeddings
    self.vanilla_embeddings = vanilla_embeddings
    self.bertweet_embeddings = bertweet_embeddings

  def __getitem__(self, idx):
    return {
      'word_embeddings': torch.tensor(self.word_embeddings[idx], dtype=torch.float),
      'simclr_embeddings': self.simclr_embeddings[idx].clone().detach(),
      'vanilla_embeddings': self.vanilla_embeddings[idx].clone().detach(),
      'bertweet_embeddings': self.bertweet_embeddings[idx].clone().detach(),
      'labels': torch.tensor(self.labels[idx], dtype=torch.float)
    }

  def __len__(self):
    return len(self.labels)

def compute_metrics(eval_pred):
  logits, labels = eval_pred
  preds = torch.sigmoid(torch.tensor(logits)).round().numpy()
  precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')
  acc = accuracy_score(labels, preds)
  return {
    'accuracy': acc,
    'f1': f1,
    'precision': precision,
    'recall': recall
  }

with open("/content/drive/MyDrive/Contrastive NLP Stuff/Data/SOTA Data/Embeddings/Twitter/train_BERTweet_sentence", "rb") as fp:
  train_simclr_embeddings = pickle.load(fp)

with open("/content/drive/MyDrive/Contrastive NLP Stuff/Data/SOTA Data/Embeddings/Twitter/test_BERTweet_sentence", "rb") as fp:
  test_simclr_embeddings = pickle.load(fp)

with open("/content/drive/MyDrive/Contrastive NLP Stuff/Data/SOTA Data/Embeddings/Twitter/val_BERTweet_sentence", "rb") as fp:
  val_simclr_embeddings = pickle.load(fp)

with open("/content/drive/MyDrive/Contrastive NLP Stuff/Data/SOTA Data/Embeddings/Twitter/train_vanilla", "rb") as fp:
  vanilla_train = pickle.load(fp)

with open("/content/drive/MyDrive/Contrastive NLP Stuff/Data/SOTA Data/Embeddings/Twitter/test_vanilla", "rb") as fp:
  vanilla_test = pickle.load(fp)

with open("/content/drive/MyDrive/Contrastive NLP Stuff/Data/SOTA Data/Embeddings/Twitter/train_vanilla", "rb") as fp:
  vanilla_val = pickle.load(fp)

with open("/content/drive/MyDrive/Contrastive NLP Stuff/Data/SOTA Data/Embeddings/Twitter/train_word", "rb") as fp:
  train_word_embeddings = pickle.load(fp)

with open("/content/drive/MyDrive/Contrastive NLP Stuff/Data/SOTA Data/Embeddings/Twitter/test_word", "rb") as fp:
  test_word_embeddings = pickle.load(fp)

with open("/content/drive/MyDrive/Contrastive NLP Stuff/Data/SOTA Data/Embeddings/Twitter/val_word", "rb") as fp:
  val_word_embeddings = pickle.load(fp)

with open("/content/drive/MyDrive/Contrastive NLP Stuff/Data/SOTA Data/Embeddings/Twitter/BERTweet_train", "rb") as fp:
  train_bertweet_embeddings = pickle.load(fp)

with open("/content/drive/MyDrive/Contrastive NLP Stuff/Data/SOTA Data/Embeddings/Twitter/BERTweet_test", "rb") as fp:
  test_bertweet_embeddings = pickle.load(fp)

with open("/content/drive/MyDrive/Contrastive NLP Stuff/Data/SOTA Data/Embeddings/Twitter/BERTweet_val", "rb") as fp:
  val_bertweet_embeddings = pickle.load(fp)

train = pd.read_csv('/content/drive/MyDrive/Contrastive NLP Stuff/Data/SOTA Data/CSV Data/Formatted Twitter/train.csv')
test = pd.read_csv('/content/drive/MyDrive/Contrastive NLP Stuff/Data/SOTA Data/CSV Data/Formatted Twitter/test.csv')
val = pd.read_csv('/content/drive/MyDrive/Contrastive NLP Stuff/Data/SOTA Data/CSV Data/Formatted Twitter/val.csv')

train_labels = train['sarcastic'].values.tolist()
test_labels = test['sarcastic'].values.tolist()
val_labels = val['sarcastic'].values.tolist()

train_dataset = SarcasmDataset(train_labels, train_word_embeddings, train_simclr_embeddings, vanilla_train, train_bertweet_embeddings)
test_dataset = SarcasmDataset(test_labels, test_word_embeddings, test_simclr_embeddings, vanilla_test, test_bertweet_embeddings)
val_dataset = SarcasmDataset(val_labels, val_word_embeddings, val_simclr_embeddings, vanilla_val, val_bertweet_embeddings)

model = FFNNClassifier(word_embed_dim=768, sentence_embed_dim=768*3, hidden_dim=1024)

optimizer = AdamW(model.parameters(), lr=5e-5)

training_args = TrainingArguments(
  output_dir='./results',
  num_train_epochs=10,
  per_device_train_batch_size=16,
  per_device_eval_batch_size=16,
  warmup_steps=500,
  weight_decay=0.01,
  logging_dir='./logs',
  evaluation_strategy="epoch",
  save_strategy="epoch",
  load_best_model_at_end=True,
)

trainer = Trainer(
  model=model,
  args=training_args,
  train_dataset=train_dataset,
  eval_dataset=val_dataset,
  compute_metrics=compute_metrics,
  optimizers=(optimizer, None),
)

trainer.train()
trainer.evaluate(test_dataset)

"""## IAC-V2:

### A1:
"""

import pandas as pd
import numpy as np
import warnings
warnings.filterwarnings('ignore')
import torch
from torch import nn
from torch.utils.data import Dataset
from transformers import Trainer,TrainingArguments
from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score
from scipy import stats

class FFNNClassifier(nn.Module):
  def __init__(self, embed_dim, hidden_dim, num_classes):
    super(FFNNClassifier, self).__init__()
    self.fc1 = nn.Linear(embed_dim, hidden_dim)
    self.relu = nn.ReLU()
    self.fc2 = nn.Linear(hidden_dim, num_classes)

  def forward(self, embeddings, labels=None):
    batch_size, seq_len, embed_dim = embeddings.shape
    embeddings = embeddings.view(batch_size * seq_len, embed_dim)
    embeddings = embeddings.float()
    out = self.fc1(embeddings)
    out = self.relu(out)
    logits = self.fc2(out)
    logits = logits.view(batch_size, seq_len, -1)
    return {'logits': logits}

class SarcasmDataset(torch.utils.data.Dataset):
  def __init__(self, embeddings, labels):
    self.embeddings = embeddings
    self.labels = labels

  def __getitem__(self, idx):
    item = {'embeddings': self.embeddings[idx], 'labels': torch.tensor(self.labels[idx])}
    return item

  def __len__(self):
    return len(self.labels)

def compute_metrics(p):
  pred, labels = p
  aggregated_logits = stats.mode(pred, axis=1)[0]
  probabilities = np.exp(aggregated_logits) / np.sum(np.exp(aggregated_logits), axis=-1, keepdims=True)
  pred = np.argmax(probabilities, axis=-1)
  accuracy = accuracy_score(y_true=labels, y_pred=pred)
  f1 = f1_score(y_true=labels, y_pred=pred, average='binary')
  precision = precision_score(y_true=labels, y_pred=pred, average='binary')
  recall = recall_score(y_true=labels, y_pred=pred, average='binary')
  return {"accuracy": accuracy, "f1": f1, "precision": precision, "recall": recall}

class CustomTrainer(Trainer):
  def compute_loss(self, model, inputs, return_outputs=False):
    embeddings = inputs.get("embeddings")
    outputs = model(embeddings)
    logits = outputs['logits']
    labels = inputs.get("labels").to(logits.device)
    probabilities = torch.nn.functional.softmax(logits, dim=-1)
    sentence_probabilities = torch.mean(probabilities, dim=1)
    sentence_logits = torch.log(sentence_probabilities)
    loss_fct = nn.CrossEntropyLoss()
    loss = loss_fct(sentence_logits, labels)
    return (loss, outputs) if return_outputs else loss

import pickle
from transformers import AutoTokenizer, TrainingArguments, Trainer, AdamW
from torch import stack

# Load word-level embeddings
with open("/content/drive/MyDrive/Contrastive NLP Stuff/Data/SOTA Data/Embeddings/IAC-V2/train_word", "rb") as fp:
  train_embeddings = pickle.load(fp)

with open("/content/drive/MyDrive/Contrastive NLP Stuff/Data/SOTA Data/Embeddings/IAC-V2/test_word", "rb") as fp:
  test_embeddings = pickle.load(fp)

with open("/content/drive/MyDrive/Contrastive NLP Stuff/Data/SOTA Data/Embeddings/IAC-V2/val_word", "rb") as fp:
  val_embeddings = pickle.load(fp)

# Load data
train = pd.read_csv('/content/drive/MyDrive/Contrastive NLP Stuff/Data/SOTA Data/CSV Data/Formatted IAC-V2/train.csv')
test = pd.read_csv('/content/drive/MyDrive/Contrastive NLP Stuff/Data/SOTA Data/CSV Data/Formatted IAC-V2/test.csv')
val = pd.read_csv('/content/drive/MyDrive/Contrastive NLP Stuff/Data/SOTA Data/CSV Data/Formatted IAC-V2/val.csv')

train_labels = train['sarcastic'].values.tolist()
test_labels = test['sarcastic'].values.tolist()
val_labels = val['sarcastic'].values.tolist()

train_dataset = SarcasmDataset(train_embeddings, train_labels)
test_dataset = SarcasmDataset(test_embeddings, test_labels)
val_dataset = SarcasmDataset(val_embeddings, val_labels)

embed_dim = 768
hidden_dim = 128
num_classes = 2

model = FFNNClassifier(embed_dim, hidden_dim, num_classes)

training_args = TrainingArguments(
  output_dir='./res', evaluation_strategy="steps", num_train_epochs=5, per_device_train_batch_size=32,
  per_device_eval_batch_size=64, warmup_steps=500, weight_decay=0.01,logging_dir='./logs4',
  load_best_model_at_end=True,
)

trainer = CustomTrainer(
  model=model, args=training_args, train_dataset=train_dataset,
  eval_dataset=val_dataset,
  compute_metrics=compute_metrics,
)

trainer.train()

# Evaluate on test dataset after training
trainer.evaluate(test_dataset)

"""### A2 RoBERTa:"""

import pandas as pd
import numpy as np
import warnings
warnings.filterwarnings('ignore')
import torch
from torch import nn
from torch.utils.data import Dataset
from transformers import Trainer,TrainingArguments
from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score

class FFNNClassifier(nn.Module):
  def __init__(self, embed_dim, hidden_dim, num_classes):
    super(FFNNClassifier, self).__init__()
    self.fc1 = nn.Linear(embed_dim, hidden_dim)
    self.relu = nn.ReLU()
    self.fc2 = nn.Linear(hidden_dim, num_classes)

  def forward(self, embeddings, labels=None):
    out = self.fc1(embeddings)
    out = self.relu(out)
    logits = self.fc2(out)
    return {'logits': logits}

class SarcasmDataset(torch.utils.data.Dataset):
  def __init__(self, embeddings, labels):
    self.embeddings = embeddings
    self.labels = labels

  def __getitem__(self, idx):
    item = {'embeddings': self.embeddings[idx], 'labels': torch.tensor(self.labels[idx])}
    return item

  def __len__(self):
    return len(self.labels)

def compute_metrics(p):
  pred, labels = p
  pred = np.argmax(pred, axis=1)

  accuracy = accuracy_score(y_true=labels, y_pred=pred)
  f1 = f1_score(labels, pred)
  precision = precision_score(labels, pred)
  recall = recall_score(labels, pred)

  return {"accuracy": accuracy, "f1_score": f1, "precision": precision, "recall": recall}

class CustomTrainer(Trainer):
  def compute_loss(self, model, inputs, return_outputs=False):
    embeddings = inputs.get("embeddings")
    outputs = model(embeddings)
    logits = outputs['logits']
    labels = inputs.get("labels").to(logits.device)
    loss_fct = nn.CrossEntropyLoss()
    loss = loss_fct(logits.view(-1, num_classes), labels.view(-1))
    return (loss, outputs) if return_outputs else loss

import pickle
from transformers import AutoTokenizer, TrainingArguments, Trainer, AdamW
from torch import stack

# Load sentence-level embeddings
with open("/content/drive/MyDrive/Contrastive NLP Stuff/Data/SOTA Data/Embeddings/IAC-V2/train_vanilla", "rb") as fp:
  vanilla_train = pickle.load(fp)

with open("/content/drive/MyDrive/Contrastive NLP Stuff/Data/SOTA Data/Embeddings/IAC-V2/test_vanilla", "rb") as fp:
  vanilla_test = pickle.load(fp)

with open("/content/drive/MyDrive/Contrastive NLP Stuff/Data/SOTA Data/Embeddings/IAC-V2/val_vanilla", "rb") as fp:
  vanilla_val = pickle.load(fp)

# Load data
train = pd.read_csv('/content/drive/MyDrive/Contrastive NLP Stuff/Data/SOTA Data/CSV Data/Formatted IAC-V2/train.csv')
test = pd.read_csv('/content/drive/MyDrive/Contrastive NLP Stuff/Data/SOTA Data/CSV Data/Formatted IAC-V2/test.csv')
val = pd.read_csv('/content/drive/MyDrive/Contrastive NLP Stuff/Data/SOTA Data/CSV Data/Formatted IAC-V2/val.csv')

train_labels = train['sarcastic'].values.tolist()
test_labels = test['sarcastic'].values.tolist()
val_labels = val['sarcastic'].values.tolist()

train_dataset = SarcasmDataset(vanilla_train, train_labels)
test_dataset = SarcasmDataset(vanilla_test, test_labels)
val_dataset = SarcasmDataset(vanilla_val, val_labels)

embed_dim = 768
hidden_dim = 128
num_classes = 2

model = FFNNClassifier(embed_dim, hidden_dim, num_classes)

training_args = TrainingArguments(
  output_dir='./res', evaluation_strategy="steps", num_train_epochs=5, per_device_train_batch_size=32,
  per_device_eval_batch_size=64, warmup_steps=500, weight_decay=0.01,logging_dir='./logs4',
  load_best_model_at_end=True,
)

trainer = CustomTrainer(
  model=model, args=training_args, train_dataset=train_dataset,
  eval_dataset=val_dataset,
  compute_metrics=compute_metrics,
)

trainer.train()

# Evaluate on test dataset after training
trainer.evaluate(test_dataset)

"""### A2 BERTweet:"""

import pandas as pd
import numpy as np
import warnings
warnings.filterwarnings('ignore')
import torch
from torch import nn
from torch.utils.data import Dataset
from transformers import Trainer,TrainingArguments
from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score

class FFNNClassifier(nn.Module):
  def __init__(self, embed_dim, hidden_dim, num_classes):
    super(FFNNClassifier, self).__init__()
    self.fc1 = nn.Linear(embed_dim, hidden_dim)
    self.relu = nn.ReLU()
    self.fc2 = nn.Linear(hidden_dim, num_classes)

  def forward(self, embeddings, labels=None):
    out = self.fc1(embeddings)
    out = self.relu(out)
    logits = self.fc2(out)
    return {'logits': logits}

class SarcasmDataset(torch.utils.data.Dataset):
  def __init__(self, embeddings, labels):
    self.embeddings = embeddings
    self.labels = labels

  def __getitem__(self, idx):
    item = {'embeddings': self.embeddings[idx], 'labels': torch.tensor(self.labels[idx])}
    return item

  def __len__(self):
    return len(self.labels)

def compute_metrics(p):
  pred, labels = p
  pred = np.argmax(pred, axis=1)

  accuracy = accuracy_score(y_true=labels, y_pred=pred)
  f1 = f1_score(labels, pred)
  precision = precision_score(labels, pred)
  recall = recall_score(labels, pred)

  return {"accuracy": accuracy, "f1_score": f1, "precision": precision, "recall": recall}

class CustomTrainer(Trainer):
  def compute_loss(self, model, inputs, return_outputs=False):
    embeddings = inputs.get("embeddings")
    outputs = model(embeddings)
    logits = outputs['logits']
    labels = inputs.get("labels").to(logits.device)
    loss_fct = nn.CrossEntropyLoss()
    loss = loss_fct(logits.view(-1, num_classes), labels.view(-1))
    return (loss, outputs) if return_outputs else loss

import pickle
from transformers import AutoTokenizer, TrainingArguments, Trainer, AdamW
from torch import stack

# Load sentence-level embeddings
with open("/content/drive/MyDrive/Contrastive NLP Stuff/Data/SOTA Data/Embeddings/IAC-V2/BERTweet_train", "rb") as fp:
  train_embeddings = pickle.load(fp)

with open("/content/drive/MyDrive/Contrastive NLP Stuff/Data/SOTA Data/Embeddings/IAC-V2/BERTweet_test", "rb") as fp:
  test_embeddings = pickle.load(fp)

with open("/content/drive/MyDrive/Contrastive NLP Stuff/Data/SOTA Data/Embeddings/IAC-V2/BERTweet_val", "rb") as fp:
  val_embeddings = pickle.load(fp)

# Load data
train = pd.read_csv('/content/drive/MyDrive/Contrastive NLP Stuff/Data/SOTA Data/CSV Data/Formatted IAC-V2/train.csv')
test = pd.read_csv('/content/drive/MyDrive/Contrastive NLP Stuff/Data/SOTA Data/CSV Data/Formatted IAC-V2/test.csv')
val = pd.read_csv('/content/drive/MyDrive/Contrastive NLP Stuff/Data/SOTA Data/CSV Data/Formatted IAC-V2/val.csv')

train_labels = train['sarcastic'].values.tolist()
test_labels = test['sarcastic'].values.tolist()
val_labels = val['sarcastic'].values.tolist()

train_dataset = SarcasmDataset(train_embeddings, train_labels)
test_dataset = SarcasmDataset(test_embeddings, test_labels)
val_dataset = SarcasmDataset(val_embeddings, val_labels)

embed_dim = 768
hidden_dim = 128
num_classes = 2

model = FFNNClassifier(embed_dim, hidden_dim, num_classes)

training_args = TrainingArguments(
  output_dir='./res', evaluation_strategy="steps", num_train_epochs=5, per_device_train_batch_size=32,
  per_device_eval_batch_size=64, warmup_steps=500, weight_decay=0.01,logging_dir='./logs4',
  load_best_model_at_end=True,
)

trainer = CustomTrainer(
  model=model, args=training_args, train_dataset=train_dataset,
  eval_dataset=val_dataset,
  compute_metrics=compute_metrics,
)

trainer.train()

# Evaluate on test dataset after training
trainer.evaluate(test_dataset)

"""### A3:"""

import pandas as pd
import numpy as np
import warnings
warnings.filterwarnings('ignore')
import torch
from torch import nn
from torch.utils.data import Dataset
from transformers import Trainer,TrainingArguments
from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score

class FFNNClassifier(nn.Module):
  def __init__(self, embed_dim, hidden_dim, num_classes):
    super(FFNNClassifier, self).__init__()
    self.fc1 = nn.Linear(embed_dim, hidden_dim)
    self.relu = nn.ReLU()
    self.fc2 = nn.Linear(hidden_dim, num_classes)

  def forward(self, embeddings, labels=None):
    out = self.fc1(embeddings)
    out = self.relu(out)
    logits = self.fc2(out)
    return {'logits': logits}

class SarcasmDataset(torch.utils.data.Dataset):
  def __init__(self, embeddings, labels):
    self.embeddings = embeddings
    self.labels = labels

  def __getitem__(self, idx):
    item = {'embeddings': self.embeddings[idx], 'labels': torch.tensor(self.labels[idx])}
    return item

  def __len__(self):
    return len(self.labels)

def compute_metrics(p):
  pred, labels = p
  pred = np.argmax(pred, axis=1)

  accuracy = accuracy_score(y_true=labels, y_pred=pred)
  f1 = f1_score(labels, pred)
  precision = precision_score(labels, pred)
  recall = recall_score(labels, pred)

  return {"accuracy": accuracy, "f1_score": f1, "precision": precision, "recall": recall}

class CustomTrainer(Trainer):
  def compute_loss(self, model, inputs, return_outputs=False):
    embeddings = inputs.get("embeddings")
    outputs = model(embeddings)
    logits = outputs['logits']
    labels = inputs.get("labels").to(logits.device)
    loss_fct = nn.CrossEntropyLoss()
    loss = loss_fct(logits.view(-1, num_classes), labels.view(-1))
    return (loss, outputs) if return_outputs else loss

import pickle
from transformers import AutoTokenizer, TrainingArguments, Trainer, AdamW
from torch import stack

# Load sentence-level embeddings
with open("/content/drive/MyDrive/Contrastive NLP Stuff/Data/SOTA Data/Embeddings/IAC-V2/train_BERTweet_sentence", "rb") as fp:
  train_embeddings = pickle.load(fp)

with open("/content/drive/MyDrive/Contrastive NLP Stuff/Data/SOTA Data/Embeddings/IAC-V2/test_BERTweet_sentence", "rb") as fp:
  test_embeddings = pickle.load(fp)

with open("/content/drive/MyDrive/Contrastive NLP Stuff/Data/SOTA Data/Embeddings/IAC-V2/val_BERTweet_sentence", "rb") as fp:
  val_embeddings = pickle.load(fp)

# Load data
train = pd.read_csv('/content/drive/MyDrive/Contrastive NLP Stuff/Data/SOTA Data/CSV Data/Formatted IAC-V2/train.csv')
test = pd.read_csv('/content/drive/MyDrive/Contrastive NLP Stuff/Data/SOTA Data/CSV Data/Formatted IAC-V2/test.csv')
val = pd.read_csv('/content/drive/MyDrive/Contrastive NLP Stuff/Data/SOTA Data/CSV Data/Formatted IAC-V2/val.csv')

train_labels = train['sarcastic'].values.tolist()
test_labels = test['sarcastic'].values.tolist()
val_labels = val['sarcastic'].values.tolist()

train_dataset = SarcasmDataset(train_embeddings, train_labels)
test_dataset = SarcasmDataset(test_embeddings, test_labels)
val_dataset = SarcasmDataset(val_embeddings, val_labels)

embed_dim = 768
hidden_dim = 128
num_classes = 2

model = FFNNClassifier(embed_dim, hidden_dim, num_classes)

training_args = TrainingArguments(
  output_dir='./res', evaluation_strategy="steps", num_train_epochs=5, per_device_train_batch_size=32,
  per_device_eval_batch_size=64, warmup_steps=500, weight_decay=0.01,logging_dir='./logs4',
  load_best_model_at_end=True,
)

trainer = CustomTrainer(
  model=model, args=training_args, train_dataset=train_dataset,
  eval_dataset=val_dataset,
  compute_metrics=compute_metrics,
)

trainer.train()

# Evaluate on test dataset after training
trainer.evaluate(test_dataset)

"""### A4:"""

import torch
from torch import nn
from torch.utils.data import Dataset
from transformers import Trainer, TrainingArguments
from sklearn.metrics import accuracy_score, precision_recall_fscore_support
import numpy as np
import pickle
from transformers import AdamW

class FFNNClassifier(nn.Module):
  def __init__(self, word_embed_dim, sentence_embed_dim, hidden_dim):
    super(FFNNClassifier, self).__init__()
    self.lstm = nn.LSTM(input_size=word_embed_dim, hidden_size=hidden_dim, batch_first=True)
    self.sentence_fc = nn.Linear(sentence_embed_dim, hidden_dim)
    self.fc = nn.Linear(hidden_dim * 2, hidden_dim)
    self.relu = nn.ReLU()
    self.classifier = nn.Linear(hidden_dim, 1)

  def forward(self, word_embeddings, simclr_embeddings, vanilla_embeddings, bertweet_embeddings, labels=None):
    _, (hidden, _) = self.lstm(word_embeddings)
    word_repr = hidden[-1]

    combined_sentence_embeddings = torch.cat([simclr_embeddings, vanilla_embeddings, bertweet_embeddings], dim=1)
    sentence_repr = self.relu(self.sentence_fc(combined_sentence_embeddings))

    combined = torch.cat([word_repr, sentence_repr], dim=1)
    combined = self.relu(self.fc(combined))

    logits = self.classifier(combined).squeeze(-1)

    loss = None
    if labels is not None:
      loss_fct = nn.BCEWithLogitsLoss()
      loss = loss_fct(logits.view(-1), labels.view(-1))

    return (loss, logits) if loss is not None else logits

class SarcasmDataset(Dataset):
  def __init__(self, labels, word_embeddings, simclr_embeddings, vanilla_embeddings, bertweet_embeddings):
    self.labels = labels
    self.word_embeddings = word_embeddings
    self.simclr_embeddings = simclr_embeddings
    self.vanilla_embeddings = vanilla_embeddings
    self.bertweet_embeddings = bertweet_embeddings

  def __getitem__(self, idx):
    return {
      'word_embeddings': torch.tensor(self.word_embeddings[idx], dtype=torch.float),
      'simclr_embeddings': self.simclr_embeddings[idx].clone().detach(),
      'vanilla_embeddings': self.vanilla_embeddings[idx].clone().detach(),
      'bertweet_embeddings': self.bertweet_embeddings[idx].clone().detach(),
      'labels': torch.tensor(self.labels[idx], dtype=torch.float)
    }

  def __len__(self):
    return len(self.labels)

def compute_metrics(eval_pred):
  logits, labels = eval_pred
  preds = torch.sigmoid(torch.tensor(logits)).round().numpy()
  precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')
  acc = accuracy_score(labels, preds)
  return {
    'accuracy': acc,
    'f1': f1,
    'precision': precision,
    'recall': recall
  }

with open("/content/drive/MyDrive/Contrastive NLP Stuff/Data/SOTA Data/Embeddings/IAC-V2/train_BERTweet_sentence", "rb") as fp:
  train_simclr_embeddings = pickle.load(fp)

with open("/content/drive/MyDrive/Contrastive NLP Stuff/Data/SOTA Data/Embeddings/IAC-V2/test_BERTweet_sentence", "rb") as fp:
  test_simclr_embeddings = pickle.load(fp)

with open("/content/drive/MyDrive/Contrastive NLP Stuff/Data/SOTA Data/Embeddings/IAC-V2/val_BERTweet_sentence", "rb") as fp:
  val_simclr_embeddings = pickle.load(fp)

with open("/content/drive/MyDrive/Contrastive NLP Stuff/Data/SOTA Data/Embeddings/IAC-V2/train_vanilla", "rb") as fp:
  vanilla_train = pickle.load(fp)

with open("/content/drive/MyDrive/Contrastive NLP Stuff/Data/SOTA Data/Embeddings/IAC-V2/test_vanilla", "rb") as fp:
  vanilla_test = pickle.load(fp)

with open("/content/drive/MyDrive/Contrastive NLP Stuff/Data/SOTA Data/Embeddings/IAC-V2/val_vanilla", "rb") as fp:
  vanilla_val = pickle.load(fp)

with open("/content/drive/MyDrive/Contrastive NLP Stuff/Data/SOTA Data/Embeddings/IAC-V2/train_word", "rb") as fp:
  train_word_embeddings = pickle.load(fp)

with open("/content/drive/MyDrive/Contrastive NLP Stuff/Data/SOTA Data/Embeddings/IAC-V2/test_word", "rb") as fp:
  test_word_embeddings = pickle.load(fp)

with open("/content/drive/MyDrive/Contrastive NLP Stuff/Data/SOTA Data/Embeddings/IAC-V2/val_word", "rb") as fp:
  val_word_embeddings = pickle.load(fp)

with open("/content/drive/MyDrive/Contrastive NLP Stuff/Data/SOTA Data/Embeddings/IAC-V2/BERTweet_train", "rb") as fp:
  train_bertweet_embeddings = pickle.load(fp)

with open("/content/drive/MyDrive/Contrastive NLP Stuff/Data/SOTA Data/Embeddings/IAC-V2/BERTweet_test", "rb") as fp:
  test_bertweet_embeddings = pickle.load(fp)

with open("/content/drive/MyDrive/Contrastive NLP Stuff/Data/SOTA Data/Embeddings/IAC-V2/BERTweet_val", "rb") as fp:
  val_bertweet_embeddings = pickle.load(fp)

train = pd.read_csv('/content/drive/MyDrive/Contrastive NLP Stuff/Data/SOTA Data/CSV Data/Formatted IAC-V2/train.csv')
test = pd.read_csv('/content/drive/MyDrive/Contrastive NLP Stuff/Data/SOTA Data/CSV Data/Formatted IAC-V2/test.csv')
val = pd.read_csv('/content/drive/MyDrive/Contrastive NLP Stuff/Data/SOTA Data/CSV Data/Formatted IAC-V2/val.csv')

train_labels = train['sarcastic'].values.tolist()
test_labels = test['sarcastic'].values.tolist()
val_labels = val['sarcastic'].values.tolist()

train_dataset = SarcasmDataset(train_labels, train_word_embeddings, train_simclr_embeddings, vanilla_train, train_bertweet_embeddings)
test_dataset = SarcasmDataset(test_labels, test_word_embeddings, test_simclr_embeddings, vanilla_test, test_bertweet_embeddings)
val_dataset = SarcasmDataset(val_labels, val_word_embeddings, val_simclr_embeddings, vanilla_val, val_bertweet_embeddings)

model = FFNNClassifier(word_embed_dim=768, sentence_embed_dim=768*3, hidden_dim=1024)

optimizer = AdamW(model.parameters(), lr=5e-5)

training_args = TrainingArguments(
  output_dir='./results',
  num_train_epochs=10,
  per_device_train_batch_size=16,
  per_device_eval_batch_size=16,
  warmup_steps=500,
  weight_decay=0.01,
  logging_dir='./logs',
  evaluation_strategy="epoch",
  save_strategy="epoch",
  load_best_model_at_end=True,
)

trainer = Trainer(
  model=model,
  args=training_args,
  train_dataset=train_dataset,
  eval_dataset=val_dataset,
  compute_metrics=compute_metrics,
  optimizers=(optimizer, None),
)

trainer.train()
trainer.evaluate(test_dataset)