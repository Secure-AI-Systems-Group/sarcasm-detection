{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# SarcasmSIGN Training Dataset:"
      ],
      "metadata": {
        "id": "IJ2r0RWjAD0x"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V1kPOU5fmMfI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d11be966-301b-4e4c-83b8-4f067ee6f204"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1kgKg-3_CmDZCT5HWUaweF30cj7Pu_3TR\n",
            "To: /content/train.txt\n",
            "100% 1.71M/1.71M [00:00<00:00, 137MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=19G6i6r_xy2d3G_j9etW_VEBQt0Nr-_CE\n",
            "To: /content/test.txt\n",
            "100% 209k/209k [00:00<00:00, 80.1MB/s]\n"
          ]
        }
      ],
      "source": [
        "# Downloading the SarcasmSIGN dataset\n",
        "!gdown https://drive.google.com/uc?id=1kgKg-3_CmDZCT5HWUaweF30cj7Pu_3TR\n",
        "!gdown https://drive.google.com/uc?id=19G6i6r_xy2d3G_j9etW_VEBQt0Nr-_CE"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Converting the test to a list of lists\n",
        "import ast\n",
        "test_list = []\n",
        "with open(\"test.txt\", 'r') as file:\n",
        "  lines = file.readlines()\n",
        "\n",
        "for i in lines:\n",
        "  test_list.append(ast.literal_eval(i))\n",
        "\n",
        "# Converting the train to a list of lists\n",
        "train_list = []\n",
        "with open(\"train.txt\", 'r') as file:\n",
        "  lines = file.readlines()\n",
        "\n",
        "for i in lines:\n",
        "  train_list.append(ast.literal_eval(i))\n",
        "\n",
        "# Combining train and test into one large dataset\n",
        "data_list = train_list + test_list"
      ],
      "metadata": {
        "id": "vOGQtsO8mkh9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_list[13459:13464]"
      ],
      "metadata": {
        "id": "pDVeewzCnTou",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c52769a-a5db-409a-bb8a-3200aa138f23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[' the big hash tag was in case you were wondering', \" I'm being sarcastic\"],\n",
              " [' and also dont get me started on tanks soon we can buy tanks as well so many things that we have at our side',\n",
              "  ' and also dont get me started on tanks soon we can buy tanks as well so many things that we have at our side'],\n",
              " [' and also dont get me started on tanks soon we can buy tanks as well so many things that we have at our side',\n",
              "  ' and also dont get me started on tanks soon we can buy tanks as well so many things that we have at our side'],\n",
              " [' and also dont get me started on tanks soon we can buy tanks as well so many things that we have at our side',\n",
              "  ' and also dont get me started on tanks soon we can buy tanks as well so many things that we have at our side'],\n",
              " [' and also dont get me started on tanks soon we can buy tanks as well so many things that we have at our side',\n",
              "  ' and also dont get me started on tanks soon we can buy tanks as well so many things that we have at our side']]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a list of list to store the data\n",
        "new_data_list = [[' best day of my life', ' worst day of my life']]\n",
        "counter = 0\n",
        "for i in range(1, 13470):\n",
        "  if(data_list[i - 1][0] == data_list[i][0]):\n",
        "    new_data_list[counter].append(data_list[i][1])\n",
        "  else:\n",
        "    counter += 1\n",
        "    new_data_list.append([data_list[i][0], data_list[i][1]])"
      ],
      "metadata": {
        "id": "dQlarYfVpKwE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Removing the occurences that have the same sarcastic text and same translations\n",
        "same_occurences = 0\n",
        "temp_list = []\n",
        "for i in new_data_list:\n",
        "  if (len(set(i)) == 1):\n",
        "    same_occurences += 1\n",
        "  else:\n",
        "    temp_list.append(i)\n",
        "\n",
        "print(same_occurences)\n",
        "new_data_list = temp_list"
      ],
      "metadata": {
        "id": "UxPo_sIf2n0W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35494677-c5d3-4082-d2a7-8a3a4259c919"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "42\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Converting it to a dictionairy\n",
        "train_dict = {}\n",
        "for i in new_data_list:\n",
        "  train_dict[i[0]] = i[1:]"
      ],
      "metadata": {
        "id": "oRdDcT0l6TOl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Some translations have duplicates so we'll remove those\n",
        "for i in train_dict.keys():\n",
        "  temp = []\n",
        "  temp.append(i)\n",
        "  train_dict[i] = list(set(train_dict[i]).difference(set(temp)))\n",
        "  train_dict[i] = list(set(train_dict[i]))"
      ],
      "metadata": {
        "id": "qo3g_Qfp8Kw7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the dictionairy as a binary file to allow for ease of getting it during the training process\n",
        "import pickle\n",
        "with open(\"SarcasmTrain\", \"wb\") as fp:\n",
        "  pickle.dump(train_dict, fp)"
      ],
      "metadata": {
        "id": "RkMgqaLyBd5N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import shutil"
      ],
      "metadata": {
        "id": "YRUHTMERB7QZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "shutil.copy(\"/content/SarcasmTrain\",\"/content/drive/MyDrive/Contrastive NLP Stuff/Data/SimCLR Model Training Data/\")"
      ],
      "metadata": {
        "id": "I6UQkDx4CKBb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}